{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bpy\n",
    "import trimesh\n",
    "import glob , os , shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "from plyfile import PlyData\n",
    "import cv2\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run in blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bpy\n",
    "import glob\n",
    "local_path = \"/Users/pataranansethpakdee/Documents/GitHub/\"\n",
    "vrm_filepath = local_path+\"3D-Waifu-Model-Generator/Datasets/3D_VRMModel/\"\n",
    "processed_filepath = local_path + \"3D-Waifu-Model-Generator/Datasets/3D_ProcessedModel/\"\n",
    "image_filepath = local_path + \"3D-Waifu-Model-Generator/Datasets/2D_Image/\"\n",
    "vrm_files = glob.glob(f'{vrm_filepath}*')\n",
    "\n",
    "def purge_orphans():\n",
    "    bpy.ops.outliner.orphans_purge(do_local_ids=True, do_linked_ids=True, do_recursive=True)\n",
    "\n",
    "def clean_scene():\n",
    "    scene = bpy.context.scene\n",
    "    bpy.data.scenes.new(\"Scene\")\n",
    "    bpy.data.scenes.remove(scene, do_unlink=True)\n",
    "    purge_orphans()\n",
    "    \n",
    "def set_new_camera():\n",
    "    scene = bpy.context.scene\n",
    "    cam_data = bpy.data.cameras.new(name=\"Camera\")\n",
    "    cam = bpy.data.objects.new(name=\"Camera\", object_data=cam_data)\n",
    "    scene.collection.objects.link(cam)\n",
    "    scene.camera = cam\n",
    "    cam.location = (0, -5.3, 0.8)\n",
    "    cam.rotation_euler = (1.5708, 0, 0) \n",
    "\n",
    "def new_scene():\n",
    "    clean_scene()\n",
    "    set_new_camera()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for file in vrm_files:\n",
    "    \n",
    "    file_name = str(count).zfill(4)\n",
    "\n",
    "    clean_scene()\n",
    "    set_new_camera()\n",
    "    bpy.ops.import_scene.vrm(filepath=file)\n",
    "    scene = bpy.context.scene\n",
    "\n",
    "\n",
    "    # Render settings\n",
    "    scene.render.filepath = f'{image_filepath}{file_name}.png'\n",
    "    scene.render.image_settings.file_format = 'PNG'\n",
    "    scene.render.resolution_x = 1920\n",
    "    scene.render.resolution_y = 1080\n",
    "\n",
    "    # Render the image\n",
    "    bpy.ops.render.render(write_still=True)\n",
    "\n",
    "    #bpy.ops.export_scene.gltf(filepath=f\"{processed_filepath}{file_name}.glb\")\n",
    "    bpy.ops.export_scene.gltf(filepath=f\"{processed_filepath}{file_name}.glb\", \n",
    "                         export_format='GLB', \n",
    "                         export_image_format='JPEG',\n",
    "                         export_image_add_webp=True,\n",
    "                         export_image_webp_fallback=True,\n",
    "                         export_texcoords=True,\n",
    "                         export_normals=True,\n",
    "                         export_materials='EXPORT',\n",
    "                         export_vertex_color='MATERIAL',\n",
    "                         export_all_vertex_colors=True)\n",
    "    count+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert glb to pointcloud (ply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this in Window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert .ply to point cloud\n",
    "ply_path = \"Datasets/3D_ProcessedModel\"\n",
    "ply_files = glob.glob(r\"Datasets/3D_ProcessedModel/*\")\n",
    "\n",
    "for file in ply_files:\n",
    "    file_name = (file.split('/')[-1]).split(\".\")[0]\n",
    "    path = f\"{ply_path}/{file_name}.glb\"\n",
    "    scene = trimesh.load(file)\n",
    "\n",
    "    # Traverse all geometries (meshes) in the scene\n",
    "    point_clouds = []\n",
    "    for name, mesh in scene.geometry.items():\n",
    "        vertices = mesh.vertices\n",
    "        colors = mesh.visual.to_color().vertex_colors  # Optional: may be texture-based\n",
    "        \n",
    "        # Sample points from the mesh\n",
    "        num_samples = 5000  # Adjust this number for more points\n",
    "        if len(vertices) > num_samples:\n",
    "            indices = np.random.choice(len(vertices), num_samples, replace=False)\n",
    "            sampled_vertices = vertices[indices]\n",
    "            sampled_colors = colors[indices]\n",
    "        else:\n",
    "            sampled_vertices = vertices\n",
    "            sampled_colors = colors\n",
    "\n",
    "        # Combine vertices and colors into a point cloud\n",
    "        point_cloud_with_color = [(*v, *c[:3]) for v, c in zip(sampled_vertices, sampled_colors)]\n",
    "        point_clouds.append(point_cloud_with_color)\n",
    "    all_points = [point for pc in point_clouds for point in pc]\n",
    "    points = np.array([p[:3] for p in all_points])\n",
    "    colors = np.array([p[3:] for p in all_points]) / 255.0  # Normalize color values\n",
    "\n",
    "    # Create Open3D point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    o3d.io.write_point_cloud(f\"Processed_Data/3D_PointCloud/{file_name}.ply\", pcd)\n",
    "#o3d.io.write_point_cloud(f\"Processed_Data/3D_PointCloud/{file_name}.pts\",pcd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = glob.glob(\"Processed_Data/3D_PointCloud/*\")\n",
    "img_path = \"Processed_Data/360deg_img/\"\n",
    "comfy_path = \"C:/Users/patar/Downloads/ComfyUI_windows_portable_nvidia/ComfyUI_windows_portable/ComfyUI/output\"\n",
    "names = []\n",
    "for file_name in file_names:\n",
    "    folder_name = file_name.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    names.append(folder_name)\n",
    "    if(not os.path.exists(img_path+folder_name)):\n",
    "        os.mkdir(img_path+folder_name)\n",
    "comfy_files = glob.glob(comfy_path+\"/*\")\n",
    "for name in names:\n",
    "    for comfy_file in comfy_files:\n",
    "        if(name==comfy_file.split(\"\\\\\")[-1].split('.')[0]):\n",
    "            shutil.move(comfy_file,img_path+name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0001'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction (SuperPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images(image_paths):\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img, (256, 256))  # Resize to match network input size\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SuperPointPretrainedNetwork'...\n",
      "remote: Enumerating objects: 81, done.\u001b[K\n",
      "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
      "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
      "remote: Total 81 (delta 12), reused 9 (delta 9), pack-reused 66 (from 1)\u001b[K\n",
      "Receiving objects: 100% (81/81), 56.61 MiB | 15.92 MiB/s, done.\n",
      "Resolving deltas: 100% (21/21), done.\n",
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/magicleap/SuperPointPretrainedNetwork\n",
    "!cd SuperPointPretrainedNetwork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperPointNet(torch.nn.Module):\n",
    "  \"\"\" Pytorch definition of SuperPoint Network. \"\"\"\n",
    "  def __init__(self):\n",
    "    super(SuperPointNet, self).__init__()\n",
    "    self.relu = torch.nn.ReLU(inplace=True)\n",
    "    self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    c1, c2, c3, c4, c5, d1 = 64, 64, 128, 128, 256, 256\n",
    "    # Shared Encoder.\n",
    "    self.conv1a = torch.nn.Conv2d(1, c1, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv1b = torch.nn.Conv2d(c1, c1, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2a = torch.nn.Conv2d(c1, c2, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2b = torch.nn.Conv2d(c2, c2, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3a = torch.nn.Conv2d(c2, c3, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3b = torch.nn.Conv2d(c3, c3, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv4a = torch.nn.Conv2d(c3, c4, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv4b = torch.nn.Conv2d(c4, c4, kernel_size=3, stride=1, padding=1)\n",
    "    # Detector Head.\n",
    "    self.convPa = torch.nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
    "    self.convPb = torch.nn.Conv2d(c5, 65, kernel_size=1, stride=1, padding=0)\n",
    "    # Descriptor Head.\n",
    "    self.convDa = torch.nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
    "    self.convDb = torch.nn.Conv2d(c5, d1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" Forward pass that jointly computes unprocessed point and descriptor\n",
    "    tensors.\n",
    "    Input\n",
    "      x: Image pytorch tensor shaped N x 1 x H x W.\n",
    "    Output\n",
    "      semi: Output point pytorch tensor shaped N x 65 x H/8 x W/8.\n",
    "      desc: Output descriptor pytorch tensor shaped N x 256 x H/8 x W/8.\n",
    "    \"\"\"\n",
    "    # Shared Encoder.\n",
    "    x = self.relu(self.conv1a(x))\n",
    "    x = self.relu(self.conv1b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv2a(x))\n",
    "    x = self.relu(self.conv2b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv3a(x))\n",
    "    x = self.relu(self.conv3b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv4a(x))\n",
    "    x = self.relu(self.conv4b(x))\n",
    "    # Detector Head.\n",
    "    cPa = self.relu(self.convPa(x))\n",
    "    semi = self.convPb(cPa)\n",
    "    # Descriptor Head.\n",
    "    cDa = self.relu(self.convDa(x))\n",
    "    desc = self.convDb(cDa)\n",
    "    dn = torch.norm(desc, p=2, dim=1) # Compute the norm.\n",
    "    desc = desc.div(torch.unsqueeze(dn, 1)) # Divide by norm to normalize.\n",
    "    return semi, desc\n",
    "class SuperPointFrontend(object):\n",
    "  \"\"\" Wrapper around pytorch net to help with pre and post image processing. \"\"\"\n",
    "  def __init__(self, weights_path, nms_dist, conf_thresh, nn_thresh,\n",
    "               cuda=False):\n",
    "    self.name = 'SuperPoint'\n",
    "    self.cuda = cuda\n",
    "    self.nms_dist = nms_dist\n",
    "    self.conf_thresh = conf_thresh\n",
    "    self.nn_thresh = nn_thresh # L2 descriptor distance for good match.\n",
    "    self.cell = 8 # Size of each output cell. Keep this fixed.\n",
    "    self.border_remove = 4 # Remove points this close to the border.\n",
    "\n",
    "    # Load the network in inference mode.\n",
    "    self.net = SuperPointNet()\n",
    "    if cuda:\n",
    "      # Train on GPU, deploy on GPU.\n",
    "      self.net.load_state_dict(torch.load(weights_path))\n",
    "      self.net = self.net.cuda()\n",
    "    else:\n",
    "      # Train on GPU, deploy on CPU.\n",
    "      self.net.load_state_dict(torch.load(weights_path,\n",
    "                               map_location=lambda storage, loc: storage))\n",
    "    self.net.eval()\n",
    "\n",
    "  def nms_fast(self, in_corners, H, W, dist_thresh):\n",
    "   \n",
    "    grid = np.zeros((H, W)).astype(int) # Track NMS data.\n",
    "    inds = np.zeros((H, W)).astype(int) # Store indices of points.\n",
    "    # Sort by confidence and round to nearest int.\n",
    "    inds1 = np.argsort(-in_corners[2,:])\n",
    "    corners = in_corners[:,inds1]\n",
    "    rcorners = corners[:2,:].round().astype(int) # Rounded corners.\n",
    "    # Check for edge case of 0 or 1 corners.\n",
    "    if rcorners.shape[1] == 0:\n",
    "      return np.zeros((3,0)).astype(int), np.zeros(0).astype(int)\n",
    "    if rcorners.shape[1] == 1:\n",
    "      out = np.vstack((rcorners, in_corners[2])).reshape(3,1)\n",
    "      return out, np.zeros((1)).astype(int)\n",
    "    # Initialize the grid.\n",
    "    for i, rc in enumerate(rcorners.T):\n",
    "      grid[rcorners[1,i], rcorners[0,i]] = 1\n",
    "      inds[rcorners[1,i], rcorners[0,i]] = i\n",
    "    # Pad the border of the grid, so that we can NMS points near the border.\n",
    "    pad = dist_thresh\n",
    "    grid = np.pad(grid, ((pad,pad), (pad,pad)), mode='constant')\n",
    "    # Iterate through points, highest to lowest conf, suppress neighborhood.\n",
    "    count = 0\n",
    "    for i, rc in enumerate(rcorners.T):\n",
    "      # Account for top and left padding.\n",
    "      pt = (rc[0]+pad, rc[1]+pad)\n",
    "      if grid[pt[1], pt[0]] == 1: # If not yet suppressed.\n",
    "        grid[pt[1]-pad:pt[1]+pad+1, pt[0]-pad:pt[0]+pad+1] = 0\n",
    "        grid[pt[1], pt[0]] = -1\n",
    "        count += 1\n",
    "    # Get all surviving -1's and return sorted array of remaining corners.\n",
    "    keepy, keepx = np.where(grid==-1)\n",
    "    keepy, keepx = keepy - pad, keepx - pad\n",
    "    inds_keep = inds[keepy, keepx]\n",
    "    out = corners[:, inds_keep]\n",
    "    values = out[-1, :]\n",
    "    inds2 = np.argsort(-values)\n",
    "    out = out[:, inds2]\n",
    "    out_inds = inds1[inds_keep[inds2]]\n",
    "    return out, out_inds\n",
    "\n",
    "  def run(self, img):\n",
    "    assert img.ndim == 2, 'Image must be grayscale.'\n",
    "    assert img.dtype == np.float32, 'Image must be float32.'\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    inp = img.copy()\n",
    "    inp = (inp.reshape(1, H, W))\n",
    "    inp = torch.from_numpy(inp)\n",
    "    inp = torch.autograd.Variable(inp).view(1, 1, H, W)\n",
    "    if self.cuda:\n",
    "      inp = inp.cuda()\n",
    "    # Forward pass of network.\n",
    "    outs = self.net.forward(inp)\n",
    "    semi, coarse_desc = outs[0], outs[1]\n",
    "    # Convert pytorch -> numpy.\n",
    "    semi = semi.data.cpu().numpy().squeeze()\n",
    "    # --- Process points.\n",
    "    dense = np.exp(semi) # Softmax.\n",
    "    dense = dense / (np.sum(dense, axis=0)+.00001) # Should sum to 1.\n",
    "    # Remove dustbin.\n",
    "    nodust = dense[:-1, :, :]\n",
    "    # Reshape to get full resolution heatmap.\n",
    "    Hc = int(H / self.cell)\n",
    "    Wc = int(W / self.cell)\n",
    "    nodust = nodust.transpose(1, 2, 0)\n",
    "    heatmap = np.reshape(nodust, [Hc, Wc, self.cell, self.cell])\n",
    "    heatmap = np.transpose(heatmap, [0, 2, 1, 3])\n",
    "    heatmap = np.reshape(heatmap, [Hc*self.cell, Wc*self.cell])\n",
    "    xs, ys = np.where(heatmap >= self.conf_thresh) # Confidence threshold.\n",
    "    if len(xs) == 0:\n",
    "      return np.zeros((3, 0)), None, None\n",
    "    pts = np.zeros((3, len(xs))) # Populate point data sized 3xN.\n",
    "    pts[0, :] = ys\n",
    "    pts[1, :] = xs\n",
    "    pts[2, :] = heatmap[xs, ys]\n",
    "    pts, _ = self.nms_fast(pts, H, W, dist_thresh=self.nms_dist) # Apply NMS.\n",
    "    inds = np.argsort(pts[2,:])\n",
    "    pts = pts[:,inds[::-1]] # Sort by confidence.\n",
    "    # Remove points along border.\n",
    "    bord = self.border_remove\n",
    "    toremoveW = np.logical_or(pts[0, :] < bord, pts[0, :] >= (W-bord))\n",
    "    toremoveH = np.logical_or(pts[1, :] < bord, pts[1, :] >= (H-bord))\n",
    "    toremove = np.logical_or(toremoveW, toremoveH)\n",
    "    pts = pts[:, ~toremove]\n",
    "    # --- Process descriptor.\n",
    "    D = coarse_desc.shape[1]\n",
    "    if pts.shape[1] == 0:\n",
    "      desc = np.zeros((D, 0))\n",
    "    else:\n",
    "      # Interpolate into descriptor map using 2D point locations.\n",
    "      samp_pts = torch.from_numpy(pts[:2, :].copy())\n",
    "      samp_pts[0, :] = (samp_pts[0, :] / (float(W)/2.)) - 1.\n",
    "      samp_pts[1, :] = (samp_pts[1, :] / (float(H)/2.)) - 1.\n",
    "      samp_pts = samp_pts.transpose(0, 1).contiguous()\n",
    "      samp_pts = samp_pts.view(1, 1, -1, 2)\n",
    "      samp_pts = samp_pts.float()\n",
    "      if self.cuda:\n",
    "        samp_pts = samp_pts.cuda()\n",
    "      desc = torch.nn.functional.grid_sample(coarse_desc, samp_pts)\n",
    "      desc = desc.data.cpu().numpy().reshape(D, -1)\n",
    "      desc /= np.linalg.norm(desc, axis=0)[np.newaxis, :]\n",
    "    return pts, desc, heatmap\n",
    "class PointTracker(object):\n",
    "  \"\"\" Class to manage a fixed memory of points and descriptors that enables\n",
    "  sparse optical flow point tracking.\n",
    "\n",
    "  Internally, the tracker stores a 'tracks' matrix sized M x (2+L), of M\n",
    "  tracks with maximum length L, where each row corresponds to:\n",
    "  row_m = [track_id_m, avg_desc_score_m, point_id_0_m, ..., point_id_L-1_m].\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, max_length, nn_thresh):\n",
    "    if max_length < 2:\n",
    "      raise ValueError('max_length must be greater than or equal to 2.')\n",
    "    self.maxl = max_length\n",
    "    self.nn_thresh = nn_thresh\n",
    "    self.all_pts = []\n",
    "    for n in range(self.maxl):\n",
    "      self.all_pts.append(np.zeros((2, 0)))\n",
    "    self.last_desc = None\n",
    "    self.tracks = np.zeros((0, self.maxl+2))\n",
    "    self.track_count = 0\n",
    "    self.max_score = 9999\n",
    "\n",
    "  def nn_match_two_way(self, desc1, desc2, nn_thresh):\n",
    "    \"\"\"\n",
    "    Performs two-way nearest neighbor matching of two sets of descriptors, such\n",
    "    that the NN match from descriptor A->B must equal the NN match from B->A.\n",
    "\n",
    "    Inputs:\n",
    "      desc1 - NxM numpy matrix of N corresponding M-dimensional descriptors.\n",
    "      desc2 - NxM numpy matrix of N corresponding M-dimensional descriptors.\n",
    "      nn_thresh - Optional descriptor distance below which is a good match.\n",
    "\n",
    "    Returns:\n",
    "      matches - 3xL numpy array, of L matches, where L <= N and each column i is\n",
    "                a match of two descriptors, d_i in image 1 and d_j' in image 2:\n",
    "                [d_i index, d_j' index, match_score]^T\n",
    "    \"\"\"\n",
    "    assert desc1.shape[0] == desc2.shape[0]\n",
    "    if desc1.shape[1] == 0 or desc2.shape[1] == 0:\n",
    "      return np.zeros((3, 0))\n",
    "    if nn_thresh < 0.0:\n",
    "      raise ValueError('\\'nn_thresh\\' should be non-negative')\n",
    "    # Compute L2 distance. Easy since vectors are unit normalized.\n",
    "    dmat = np.dot(desc1.T, desc2)\n",
    "    dmat = np.sqrt(2-2*np.clip(dmat, -1, 1))\n",
    "    # Get NN indices and scores.\n",
    "    idx = np.argmin(dmat, axis=1)\n",
    "    scores = dmat[np.arange(dmat.shape[0]), idx]\n",
    "    # Threshold the NN matches.\n",
    "    keep = scores < nn_thresh\n",
    "    # Check if nearest neighbor goes both directions and keep those.\n",
    "    idx2 = np.argmin(dmat, axis=0)\n",
    "    keep_bi = np.arange(len(idx)) == idx2[idx]\n",
    "    keep = np.logical_and(keep, keep_bi)\n",
    "    idx = idx[keep]\n",
    "    scores = scores[keep]\n",
    "    # Get the surviving point indices.\n",
    "    m_idx1 = np.arange(desc1.shape[1])[keep]\n",
    "    m_idx2 = idx\n",
    "    # Populate the final 3xN match data structure.\n",
    "    matches = np.zeros((3, int(keep.sum())))\n",
    "    matches[0, :] = m_idx1\n",
    "    matches[1, :] = m_idx2\n",
    "    matches[2, :] = scores\n",
    "    return matches\n",
    "\n",
    "  def get_offsets(self):\n",
    "    \"\"\" Iterate through list of points and accumulate an offset value. Used to\n",
    "    index the global point IDs into the list of points.\n",
    "\n",
    "    Returns\n",
    "      offsets - N length array with integer offset locations.\n",
    "    \"\"\"\n",
    "    # Compute id offsets.\n",
    "    offsets = []\n",
    "    offsets.append(0)\n",
    "    for i in range(len(self.all_pts)-1): # Skip last camera size, not needed.\n",
    "      offsets.append(self.all_pts[i].shape[1])\n",
    "    offsets = np.array(offsets)\n",
    "    offsets = np.cumsum(offsets)\n",
    "    return offsets\n",
    "\n",
    "  def update(self, pts, desc):\n",
    "    \"\"\" Add a new set of point and descriptor observations to the tracker.\n",
    "\n",
    "    Inputs\n",
    "      pts - 3xN numpy array of 2D point observations.\n",
    "      desc - DxN numpy array of corresponding D dimensional descriptors.\n",
    "    \"\"\"\n",
    "    if pts is None or desc is None:\n",
    "      print('PointTracker: Warning, no points were added to tracker.')\n",
    "      return\n",
    "    assert pts.shape[1] == desc.shape[1]\n",
    "    # Initialize last_desc.\n",
    "    if self.last_desc is None:\n",
    "      self.last_desc = np.zeros((desc.shape[0], 0))\n",
    "    # Remove oldest points, store its size to update ids later.\n",
    "    remove_size = self.all_pts[0].shape[1]\n",
    "    self.all_pts.pop(0)\n",
    "    self.all_pts.append(pts)\n",
    "    # Remove oldest point in track.\n",
    "    self.tracks = np.delete(self.tracks, 2, axis=1)\n",
    "    # Update track offsets.\n",
    "    for i in range(2, self.tracks.shape[1]):\n",
    "      self.tracks[:, i] -= remove_size\n",
    "    self.tracks[:, 2:][self.tracks[:, 2:] < -1] = -1\n",
    "    offsets = self.get_offsets()\n",
    "    # Add a new -1 column.\n",
    "    self.tracks = np.hstack((self.tracks, -1*np.ones((self.tracks.shape[0], 1))))\n",
    "    # Try to append to existing tracks.\n",
    "    matched = np.zeros((pts.shape[1])).astype(bool)\n",
    "    matches = self.nn_match_two_way(self.last_desc, desc, self.nn_thresh)\n",
    "    for match in matches.T:\n",
    "      # Add a new point to it's matched track.\n",
    "      id1 = int(match[0]) + offsets[-2]\n",
    "      id2 = int(match[1]) + offsets[-1]\n",
    "      found = np.argwhere(self.tracks[:, -2] == id1)\n",
    "      if found.shape[0] > 0:\n",
    "        matched[int(match[1])] = True\n",
    "        row = int(found)\n",
    "        self.tracks[row, -1] = id2\n",
    "        if self.tracks[row, 1] == self.max_score:\n",
    "          # Initialize track score.\n",
    "          self.tracks[row, 1] = match[2]\n",
    "        else:\n",
    "          # Update track score with running average.\n",
    "          # NOTE(dd): this running average can contain scores from old matches\n",
    "          #           not contained in last max_length track points.\n",
    "          track_len = (self.tracks[row, 2:] != -1).sum() - 1.\n",
    "          frac = 1. / float(track_len)\n",
    "          self.tracks[row, 1] = (1.-frac)*self.tracks[row, 1] + frac*match[2]\n",
    "    # Add unmatched tracks.\n",
    "    new_ids = np.arange(pts.shape[1]) + offsets[-1]\n",
    "    new_ids = new_ids[~matched]\n",
    "    new_tracks = -1*np.ones((new_ids.shape[0], self.maxl + 2))\n",
    "    new_tracks[:, -1] = new_ids\n",
    "    new_num = new_ids.shape[0]\n",
    "    new_trackids = self.track_count + np.arange(new_num)\n",
    "    new_tracks[:, 0] = new_trackids\n",
    "    new_tracks[:, 1] = self.max_score*np.ones(new_ids.shape[0])\n",
    "    self.tracks = np.vstack((self.tracks, new_tracks))\n",
    "    self.track_count += new_num # Update the track count.\n",
    "    # Remove empty tracks.\n",
    "    keep_rows = np.any(self.tracks[:, 2:] >= 0, axis=1)\n",
    "    self.tracks = self.tracks[keep_rows, :]\n",
    "    # Store the last descriptors.\n",
    "    self.last_desc = desc.copy()\n",
    "    return\n",
    "\n",
    "  def get_tracks(self, min_length):\n",
    "    \"\"\" Retrieve point tracks of a given minimum length.\n",
    "    Input\n",
    "      min_length - integer >= 1 with minimum track length\n",
    "    Output\n",
    "      returned_tracks - M x (2+L) sized matrix storing track indices, where\n",
    "        M is the number of tracks and L is the maximum track length.\n",
    "    \"\"\"\n",
    "    if min_length < 1:\n",
    "      raise ValueError('\\'min_length\\' too small.')\n",
    "    valid = np.ones((self.tracks.shape[0])).astype(bool)\n",
    "    good_len = np.sum(self.tracks[:, 2:] != -1, axis=1) >= min_length\n",
    "    # Remove tracks which do not have an observation in most recent frame.\n",
    "    not_headless = (self.tracks[:, -1] != -1)\n",
    "    keepers = np.logical_and.reduce((valid, good_len, not_headless))\n",
    "    returned_tracks = self.tracks[keepers, :].copy()\n",
    "    return returned_tracks\n",
    "\n",
    "  def draw_tracks(self, out, tracks):\n",
    "    \"\"\" Visualize tracks all overlayed on a single image.\n",
    "    Inputs\n",
    "      out - numpy uint8 image sized HxWx3 upon which tracks are overlayed.\n",
    "      tracks - M x (2+L) sized matrix storing track info.\n",
    "    \"\"\"\n",
    "    # Store the number of points per camera.\n",
    "    pts_mem = self.all_pts\n",
    "    N = len(pts_mem) # Number of cameras/images.\n",
    "    # Get offset ids needed to reference into pts_mem.\n",
    "    offsets = self.get_offsets()\n",
    "    # Width of track and point circles to be drawn.\n",
    "    stroke = 1\n",
    "    # Iterate through each track and draw it.\n",
    "    for track in tracks:\n",
    "      clr = myjet[int(np.clip(np.floor(track[1]*10), 0, 9)), :]*255\n",
    "      for i in range(N-1):\n",
    "        if track[i+2] == -1 or track[i+3] == -1:\n",
    "          continue\n",
    "        offset1 = offsets[i]\n",
    "        offset2 = offsets[i+1]\n",
    "        idx1 = int(track[i+2]-offset1)\n",
    "        idx2 = int(track[i+3]-offset2)\n",
    "        pt1 = pts_mem[i][:2, idx1]\n",
    "        pt2 = pts_mem[i+1][:2, idx2]\n",
    "        p1 = (int(round(pt1[0])), int(round(pt1[1])))\n",
    "        p2 = (int(round(pt2[0])), int(round(pt2[1])))\n",
    "        cv2.line(out, p1, p2, clr, thickness=stroke, lineType=16)\n",
    "        # Draw end points of each track.\n",
    "        if i == N-2:\n",
    "          clr2 = (255, 0, 0)\n",
    "          cv2.circle(out, p2, stroke, clr2, -1, lineType=16)\n",
    "class VideoStreamer(object):\n",
    "  \"\"\" Class to help process image streams. Three types of possible inputs:\"\n",
    "    1.) USB Webcam.\n",
    "    2.) A directory of images (files in directory matching 'img_glob').\n",
    "    3.) A video file, such as an .mp4 or .avi file.\n",
    "  \"\"\"\n",
    "  def __init__(self, basedir, height, width):\n",
    "    self.cap = []\n",
    "    self.camera = False\n",
    "    self.listing = []\n",
    "    self.sizer = [height, width]\n",
    "    self.i = 0\n",
    "\n",
    "    self.maxlen = 1000000\n",
    "    search = os.path.join(basedir, '*')\n",
    "    self.listing = glob.glob(search)\n",
    "    self.listing.sort()\n",
    "    self.listing = self.listing[::1]\n",
    "    self.maxlen = len(self.listing)\n",
    "    if self.maxlen == 0:\n",
    "      raise IOError('No images were found (maybe bad \\'--img_glob\\' parameter?)')\n",
    "\n",
    "  def read_image(self, impath, img_size):\n",
    "    grayim = cv2.imread(impath, 0)\n",
    "    if grayim is None:\n",
    "      raise Exception('Error reading image %s' % impath)\n",
    "    # Image is resized via opencv.\n",
    "    interp = cv2.INTER_AREA\n",
    "    grayim = cv2.resize(grayim, (img_size[1], img_size[0]), interpolation=interp)\n",
    "    grayim = (grayim.astype('float32') / 255.)\n",
    "    return grayim\n",
    "\n",
    "  def next_frame(self):\n",
    "    \"\"\" Return the next frame, and increment internal counter.\n",
    "    Returns\n",
    "       image: Next H x W image.\n",
    "       status: True or False depending whether image was loaded.\n",
    "    \"\"\"\n",
    "    if self.i == self.maxlen:\n",
    "      return (None, False)\n",
    "    if self.camera:\n",
    "      ret, input_image = self.cap.read()\n",
    "      if ret is False:\n",
    "        print('VideoStreamer: Cannot get image from camera (maybe bad --camid?)')\n",
    "        return (None, False)\n",
    "      if self.video_file:\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.listing[self.i])\n",
    "      input_image = cv2.resize(input_image, (self.sizer[1], self.sizer[0]),\n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "      input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2GRAY)\n",
    "      input_image = input_image.astype('float')/255.0\n",
    "    else:\n",
    "      image_file = self.listing[self.i]\n",
    "      input_image = self.read_image(image_file, self.sizer)\n",
    "    # Increment internal counter.\n",
    "    self.i = self.i + 1\n",
    "    input_image = input_image.astype('float32')\n",
    "    return (input_image, True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # Parse command line arguments.\n",
    "myjet = np.array([[0.        , 0.        , 0.5       ],\n",
    "                  [0.        , 0.        , 0.99910873],\n",
    "                  [0.        , 0.37843137, 1.        ],\n",
    "                  [0.        , 0.83333333, 1.        ],\n",
    "                  [0.30044276, 1.        , 0.66729918],\n",
    "                  [0.66729918, 1.        , 0.30044276],\n",
    "                  [1.        , 0.90123457, 0.        ],\n",
    "                  [1.        , 0.48002905, 0.        ],\n",
    "                  [0.99910873, 0.07334786, 0.        ],\n",
    "                  [0.5       , 0.        , 0.        ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/f2jns9z13n5cnm9qgvvhrp3c0000gn/T/ipykernel_13480/4009406419.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(weights_path,\n"
     ]
    }
   ],
   "source": [
    "input_path = 'Processed_Data/360deg_img'\n",
    "output_path = 'Processed_Data/Feature Map'\n",
    "input_dirs = glob.glob(input_path+'/*')\n",
    "img_w = 512\n",
    "img_h = 512\n",
    "weight_path = 'SuperPointPretrainedNetwork/superpoint_v1.pth'\n",
    "dist = 4\n",
    "conf_thresh=0.015\n",
    "nn_thresh = 0.7\n",
    "GPU = False\n",
    "fe = SuperPointFrontend(weights_path=weight_path,\n",
    "                        nms_dist=dist,\n",
    "                        conf_thresh=conf_thresh,\n",
    "                        nn_thresh=nn_thresh,\n",
    "                        cuda=GPU)\n",
    "tracker = PointTracker(5, nn_thresh=fe.nn_thresh)\n",
    "\n",
    "def ExtractFeature(input_dirs,output_dirs):\n",
    "    vs = VideoStreamer(input_dirs, 512, 512)\n",
    "\n",
    "    win = 'SuperPoint Tracker'\n",
    "    # Font parameters for visualizaton.\n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    font_clr = (255, 255, 255)\n",
    "    font_pt = (4, 12)\n",
    "    font_sc = 0.4\n",
    "\n",
    "    if not os.path.exists(output_dirs):\n",
    "        os.makedirs(output_dirs)\n",
    "    while True:\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Get a new image.\n",
    "        img, status = vs.next_frame()\n",
    "        if status is False:\n",
    "            break\n",
    "\n",
    "        # Get points and descriptors.\n",
    "        pts, desc, heatmap = fe.run(img)\n",
    "\n",
    "        # Add points and descriptors to the tracker.\n",
    "        tracker.update(pts, desc)\n",
    "\n",
    "        # Get tracks for points which were match successfully across all frames.\n",
    "        tracks = tracker.get_tracks(2)\n",
    "\n",
    "        # Primary output - Show point tracks overlayed on top of input image.\n",
    "        out1 = (np.dstack((img, img, img)) * 255.).astype('uint8')\n",
    "        tracks[:, 1] /= float(fe.nn_thresh) # Normalize track scores to [0,1].\n",
    "        tracker.draw_tracks(out1, tracks)\n",
    "        if True:\n",
    "            cv2.putText(out1, 'Point Tracks', font_pt, font, font_sc, font_clr, lineType=16)\n",
    "\n",
    "        # Extra output -- Show current point detections.\n",
    "        out2 = (np.dstack((img, img, img)) * 255.).astype('uint8')\n",
    "        for pt in pts.T:\n",
    "            pt1 = (int(round(pt[0])), int(round(pt[1])))\n",
    "            cv2.circle(out2, pt1, 1, (0, 255, 0), -1, lineType=16)\n",
    "        cv2.putText(out2, 'Raw Point Detections', font_pt, font, font_sc, font_clr, lineType=16)\n",
    "\n",
    "        # Extra output -- Show the point confidence heatmap.\n",
    "        if heatmap is not None:\n",
    "            min_conf = 0.001\n",
    "            heatmap[heatmap < min_conf] = min_conf\n",
    "            heatmap = -np.log(heatmap)\n",
    "            heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + .00001)\n",
    "            out3 = myjet[np.round(np.clip(heatmap*10, 0, 9)).astype('int'), :]\n",
    "            out3 = (out3*255).astype('uint8')\n",
    "        else:\n",
    "            out3 = np.zeros_like(out2)\n",
    "        cv2.putText(out3, 'Raw Point Confidences', font_pt, font, font_sc, font_clr, lineType=16)\n",
    "\n",
    "        # Resize final output.\n",
    "        if False:\n",
    "            out = np.hstack((out1, out2, out3))\n",
    "            out = cv2.resize(out, (3*2*512, 2*512))\n",
    "        else:\n",
    "            out = cv2.resize(out2, (2*512, 2*512))\n",
    "\n",
    "        # Optionally write images to disk.\n",
    "        if True:\n",
    "            out_file = os.path.join(output_dirs, 'frame_%05d.png' % vs.i)\n",
    "            cv2.imwrite(out_file, out)\n",
    "\n",
    "\n",
    "    print(f'==> Finshed {output_dirs}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Processing Image Directory Input.\n",
      "==> Will write outputs to Processed_Data/Feature Map/0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/f2jns9z13n5cnm9qgvvhrp3c0000gn/T/ipykernel_13480/4009406419.py:299: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  row = int(found)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Finshed Processed_Data/Feature Map/0000.\n",
      "==> Processing Image Directory Input.\n",
      "==> Will write outputs to Processed_Data/Feature Map/0001\n",
      "==> Finshed Processed_Data/Feature Map/0001.\n"
     ]
    }
   ],
   "source": [
    "for dirPath in input_dirs:\n",
    "    folderName = dirPath.split(\"/\")[-1]\n",
    "    outputPath = os.path.join(output_path,folderName)\n",
    "    ExtractFeature(dirPath,outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s1/f2jns9z13n5cnm9qgvvhrp3c0000gn/T/ipykernel_13480/4009406419.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(weights_path,\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def match_images_across_multiple(img_folder,outputFolder, fe, nn_thresh=0.7, save_matches=False):\n",
    "    # Get all image paths from the folder\n",
    "    img_paths = sorted(glob.glob((img_folder+'/*')))\n",
    "    num_images = len(img_paths)\n",
    "    assert num_images >= 2, \"Need at least two images for matching.\"\n",
    "\n",
    "    all_matches = []  # To store matches across all images\n",
    "    if not os.path.exists(outputFolder):\n",
    "        os.makedirs(outputFolder)\n",
    "    # Iterate over image pairs (i, i+1)\n",
    "    for i in range(num_images - 1):\n",
    "\n",
    "        # Read consecutive images\n",
    "        img1 = cv2.imread(img_paths[i], cv2.IMREAD_GRAYSCALE)\n",
    "        img2 = cv2.imread(img_paths[i+1], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img1 = img1.astype(np.float32) / 255.0\n",
    "        img2 = img2.astype(np.float32) / 255.0\n",
    "\n",
    "        # Extract keypoints and descriptors using SuperPoint\n",
    "        pts1, desc1, _ = fe.run(img1)\n",
    "        pts2, desc2, _ = fe.run(img2)\n",
    "\n",
    "        # Match descriptors between img1 and img2\n",
    "        good_matches = feature_match(desc1, desc2, nn_thresh)\n",
    "        \n",
    "        # Visualize matches if desired\n",
    "        if save_matches:\n",
    "            visualize_and_save(img1, img2, pts1, pts2, good_matches, i,outputFolder)\n",
    "        match_data = [(m.queryIdx, m.trainIdx, m.distance) for m in good_matches]\n",
    "\n",
    "        # Save matches for this image pair as a .npy file\n",
    "        match_file = os.path.join(outputFolder, f'matches_{i}_{i + 1}.npy')\n",
    "        np.save(match_file, match_data)\n",
    "\n",
    "        all_matches.append(good_matches)\n",
    "\n",
    "    return all_matches\n",
    "\n",
    "def feature_match(desc1, desc2, nn_thresh=0.7):\n",
    "    \"\"\"Performs nearest neighbor matching between two descriptors.\"\"\"\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    desc1 = desc1.T\n",
    "    desc2 = desc2.T\n",
    "    matches = bf.match(desc1, desc2)\n",
    "    \n",
    "    # Filter good matches\n",
    "    good_matches = [m for m in matches if m.distance < nn_thresh]\n",
    "    return good_matches\n",
    "\n",
    "def visualize_and_save(img1, img2, pts1, pts2, matches, img_idx,outputFolder):\n",
    "    \"\"\"\n",
    "    Visualize and save the matches between two images.\n",
    "\n",
    "    Args:\n",
    "        img1: First image (float32 grayscale).\n",
    "        img2: Second image (float32 grayscale).\n",
    "        pts1: Keypoints from the first image.\n",
    "        pts2: Keypoints from the second image.\n",
    "        matches: Good matches between descriptors of the two images.\n",
    "        img_idx: Index of the image pair being processed.\n",
    "    \"\"\"\n",
    "    # Convert images back to uint8 for visualization\n",
    "    img1_u8 = (img1 * 255).astype(np.uint8)\n",
    "    img2_u8 = (img2 * 255).astype(np.uint8)\n",
    "\n",
    "    # Convert keypoints for visualization\n",
    "    kp1 = [cv2.KeyPoint(pt[0], pt[1], 1) for pt in pts1.T]\n",
    "    kp2 = [cv2.KeyPoint(pt[0], pt[1], 1) for pt in pts2.T]\n",
    "\n",
    "    # Draw matches\n",
    "    out_img = cv2.drawMatches(img1_u8, kp1, img2_u8, kp2, matches, None)\n",
    "\n",
    "    # Save output image\n",
    "    out_file = f'{outputFolder}/matches_{img_idx}_{img_idx+1}.png'\n",
    "    cv2.imwrite(out_file, out_img)\n",
    "\n",
    "\n",
    "# Define parameters and run matching across 21 images\n",
    "img_folder = glob.glob('Processed_Data/360deg_img/*')\n",
    "weight_path = 'SuperPointPretrainedNetwork/superpoint_v1.pth'\n",
    "dist = 4\n",
    "conf_thresh = 0.015\n",
    "nn_thresh = 0.7\n",
    "GPU = False\n",
    "\n",
    "# Initialize SuperPointFrontend\n",
    "fe = SuperPointFrontend(weights_path=weight_path,\n",
    "                        nms_dist=dist,\n",
    "                        conf_thresh=conf_thresh,\n",
    "                        nn_thresh=nn_thresh,\n",
    "                        cuda=GPU)\n",
    "\n",
    "# Run matching across 21 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for folder in img_folder:\n",
    "    name = folder.split('/')[-1]\n",
    "    outputFolder = os.path.join('Processed_Data/Feature Match',name)\n",
    "    matched = match_images_across_multiple(folder, outputFolder, fe, nn_thresh, save_matches=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matched[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SfM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W20240927 23:02:09.202757 0x2074e3ac0 feature_extraction.cc:406] Your current options use the maximum number of threads on the machine to extract features. Extracting SIFT features on the CPU can consume a lot of RAM per thread for large images. Consider reducing the maximum image size and/or the first octave or manually limit the number of extraction threads. Ignore this warning, if your machine has sufficient memory for the current settings.\n",
      "I20240927 23:02:09.203234 0x174bc7000 misc.cc:198] \n",
      "==============================================================================\n",
      "Feature extraction\n",
      "==============================================================================\n",
      "I20240927 23:02:09.203423 0x1753fb000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203434 0x175487000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203443 0x175513000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203455 0x17559f000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203465 0x17562b000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203476 0x1756b7000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203499 0x1757cf000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203502 0x175743000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203514 0x17585b000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203541 0x175973000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203548 0x1758e7000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203562 0x1759ff000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203569 0x175a8b000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203593 0x175b17000 sift.cc:722] Creating SIFT CPU feature extractor\n",
      "I20240927 23:02:09.203690 0x175ba3000 feature_extraction.cc:257] Processed file [1/21]\n",
      "I20240927 23:02:09.203699 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00001_.png\n",
      "I20240927 23:02:09.203702 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203705 0x175ba3000 feature_extraction.cc:257] Processed file [2/21]\n",
      "I20240927 23:02:09.203709 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00002_.png\n",
      "I20240927 23:02:09.203711 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203763 0x175ba3000 feature_extraction.cc:257] Processed file [3/21]\n",
      "I20240927 23:02:09.203773 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00003_.png\n",
      "I20240927 23:02:09.203778 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203791 0x175ba3000 feature_extraction.cc:257] Processed file [4/21]\n",
      "I20240927 23:02:09.203794 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00004_.png\n",
      "I20240927 23:02:09.203797 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203800 0x175ba3000 feature_extraction.cc:257] Processed file [5/21]\n",
      "I20240927 23:02:09.203803 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00005_.png\n",
      "I20240927 23:02:09.203806 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203834 0x175ba3000 feature_extraction.cc:257] Processed file [6/21]\n",
      "I20240927 23:02:09.203844 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00006_.png\n",
      "I20240927 23:02:09.203849 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203856 0x175ba3000 feature_extraction.cc:257] Processed file [7/21]\n",
      "I20240927 23:02:09.203859 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00007_.png\n",
      "I20240927 23:02:09.203863 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203867 0x175ba3000 feature_extraction.cc:257] Processed file [8/21]\n",
      "I20240927 23:02:09.203869 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00008_.png\n",
      "I20240927 23:02:09.203872 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203875 0x175ba3000 feature_extraction.cc:257] Processed file [9/21]\n",
      "I20240927 23:02:09.203878 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00009_.png\n",
      "I20240927 23:02:09.203881 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203907 0x175ba3000 feature_extraction.cc:257] Processed file [10/21]\n",
      "I20240927 23:02:09.203910 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00010_.png\n",
      "I20240927 23:02:09.203913 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203917 0x175ba3000 feature_extraction.cc:257] Processed file [11/21]\n",
      "I20240927 23:02:09.203919 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00011_.png\n",
      "I20240927 23:02:09.203922 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203962 0x175ba3000 feature_extraction.cc:257] Processed file [12/21]\n",
      "I20240927 23:02:09.203974 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00012_.png\n",
      "I20240927 23:02:09.203980 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203986 0x175ba3000 feature_extraction.cc:257] Processed file [13/21]\n",
      "I20240927 23:02:09.203990 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00013_.png\n",
      "I20240927 23:02:09.203993 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.203999 0x175ba3000 feature_extraction.cc:257] Processed file [14/21]\n",
      "I20240927 23:02:09.204002 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00014_.png\n",
      "I20240927 23:02:09.204006 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.204009 0x175ba3000 feature_extraction.cc:257] Processed file [15/21]\n",
      "I20240927 23:02:09.204012 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00015_.png\n",
      "I20240927 23:02:09.204015 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.204019 0x175ba3000 feature_extraction.cc:257] Processed file [16/21]\n",
      "I20240927 23:02:09.204022 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00016_.png\n",
      "I20240927 23:02:09.204025 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.204028 0x175ba3000 feature_extraction.cc:257] Processed file [17/21]\n",
      "I20240927 23:02:09.204030 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00017_.png\n",
      "I20240927 23:02:09.204033 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.204036 0x175ba3000 feature_extraction.cc:257] Processed file [18/21]\n",
      "I20240927 23:02:09.204039 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00018_.png\n",
      "I20240927 23:02:09.204042 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.204046 0x175ba3000 feature_extraction.cc:257] Processed file [19/21]\n",
      "I20240927 23:02:09.204049 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00019_.png\n",
      "I20240927 23:02:09.204051 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.204080 0x175ba3000 feature_extraction.cc:257] Processed file [20/21]\n",
      "I20240927 23:02:09.204092 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00020_.png\n",
      "I20240927 23:02:09.204095 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.204098 0x175ba3000 feature_extraction.cc:257] Processed file [21/21]\n",
      "I20240927 23:02:09.204101 0x175ba3000 feature_extraction.cc:260]   Name:            0000.png_00021_.png\n",
      "I20240927 23:02:09.204106 0x175ba3000 feature_extraction.cc:264]   SKIP: Features for image already extracted.\n",
      "I20240927 23:02:09.204506 0x174bc7000 timer.cc:91] Elapsed time: 0.000 [minutes]\n",
      "I20240927 23:02:09.206841 0x174bc7000 misc.cc:198] \n",
      "==============================================================================\n",
      "Feature matching\n",
      "==============================================================================\n",
      "I20240927 23:02:09.206992 0x174cdf000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.206987 0x174c53000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207040 0x174df7000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207017 0x174d6b000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207049 0x174e83000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207062 0x174f0f000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207072 0x174f9b000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207080 0x175027000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207100 0x17513f000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207178 0x1750b3000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207196 0x1751cb000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207200 0x175257000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207219 0x17536f000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207221 0x1752e3000 sift.cc:1457] Creating SIFT CPU feature matcher\n",
      "I20240927 23:02:09.207431 0x174bc7000 pairing.cc:391] Generating sequential image pairs...\n",
      "I20240927 23:02:09.207449 0x174bc7000 pairing.cc:436] Matching image [1/21]\n",
      "I20240927 23:02:09.223119 0x174bc7000 feature_matching.cc:46]  in 0.016s\n",
      "I20240927 23:02:09.223167 0x174bc7000 pairing.cc:436] Matching image [2/21]\n",
      "I20240927 23:02:09.235191 0x174bc7000 feature_matching.cc:46]  in 0.012s\n",
      "I20240927 23:02:09.235268 0x174bc7000 pairing.cc:436] Matching image [3/21]\n",
      "I20240927 23:02:09.246251 0x174bc7000 feature_matching.cc:46]  in 0.011s\n",
      "I20240927 23:02:09.246284 0x174bc7000 pairing.cc:436] Matching image [4/21]\n",
      "I20240927 23:02:09.256422 0x174bc7000 feature_matching.cc:46]  in 0.010s\n",
      "I20240927 23:02:09.256507 0x174bc7000 pairing.cc:436] Matching image [5/21]\n",
      "I20240927 23:02:09.266002 0x174bc7000 feature_matching.cc:46]  in 0.010s\n",
      "I20240927 23:02:09.266071 0x174bc7000 pairing.cc:436] Matching image [6/21]\n",
      "I20240927 23:02:09.274286 0x174bc7000 feature_matching.cc:46]  in 0.008s\n",
      "I20240927 23:02:09.274339 0x174bc7000 pairing.cc:436] Matching image [7/21]\n",
      "I20240927 23:02:09.282040 0x174bc7000 feature_matching.cc:46]  in 0.008s\n",
      "I20240927 23:02:09.282093 0x174bc7000 pairing.cc:436] Matching image [8/21]\n",
      "I20240927 23:02:09.290464 0x174bc7000 feature_matching.cc:46]  in 0.008s\n",
      "I20240927 23:02:09.290514 0x174bc7000 pairing.cc:436] Matching image [9/21]\n",
      "I20240927 23:02:09.299528 0x174bc7000 feature_matching.cc:46]  in 0.009s\n",
      "I20240927 23:02:09.299616 0x174bc7000 pairing.cc:436] Matching image [10/21]\n",
      "I20240927 23:02:09.309473 0x174bc7000 feature_matching.cc:46]  in 0.010s\n",
      "I20240927 23:02:09.309528 0x174bc7000 pairing.cc:436] Matching image [11/21]\n",
      "I20240927 23:02:09.319157 0x174bc7000 feature_matching.cc:46]  in 0.010s\n",
      "I20240927 23:02:09.319262 0x174bc7000 pairing.cc:436] Matching image [12/21]\n",
      "I20240927 23:02:09.328583 0x174bc7000 feature_matching.cc:46]  in 0.009s\n",
      "I20240927 23:02:09.328625 0x174bc7000 pairing.cc:436] Matching image [13/21]\n",
      "I20240927 23:02:09.337435 0x174bc7000 feature_matching.cc:46]  in 0.009s\n",
      "I20240927 23:02:09.337467 0x174bc7000 pairing.cc:436] Matching image [14/21]\n",
      "I20240927 23:02:09.345554 0x174bc7000 feature_matching.cc:46]  in 0.008s\n",
      "I20240927 23:02:09.345602 0x174bc7000 pairing.cc:436] Matching image [15/21]\n",
      "I20240927 23:02:09.354111 0x174bc7000 feature_matching.cc:46]  in 0.009s\n",
      "I20240927 23:02:09.354134 0x174bc7000 pairing.cc:436] Matching image [16/21]\n",
      "I20240927 23:02:09.363285 0x174bc7000 feature_matching.cc:46]  in 0.009s\n",
      "I20240927 23:02:09.363349 0x174bc7000 pairing.cc:436] Matching image [17/21]\n",
      "I20240927 23:02:09.372002 0x174bc7000 feature_matching.cc:46]  in 0.009s\n",
      "I20240927 23:02:09.372027 0x174bc7000 pairing.cc:436] Matching image [18/21]\n",
      "I20240927 23:02:09.381404 0x174bc7000 feature_matching.cc:46]  in 0.009s\n",
      "I20240927 23:02:09.381494 0x174bc7000 pairing.cc:436] Matching image [19/21]\n",
      "I20240927 23:02:09.391017 0x174bc7000 feature_matching.cc:46]  in 0.010s\n",
      "I20240927 23:02:09.391076 0x174bc7000 pairing.cc:436] Matching image [20/21]\n",
      "I20240927 23:02:09.400405 0x174bc7000 feature_matching.cc:46]  in 0.009s\n",
      "I20240927 23:02:09.400434 0x174bc7000 pairing.cc:436] Matching image [21/21]\n",
      "I20240927 23:02:09.400440 0x174bc7000 feature_matching.cc:46]  in 0.000s\n",
      "I20240927 23:02:09.400443 0x174bc7000 timer.cc:91] Elapsed time: 0.003 [minutes]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'pycolmap.Reconstruction' object has no attribute 'sparse_reconstruction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m ply_point_cloud \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath_to_your_point_cloud.ply\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Optional, if used\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Run SfM pipeline\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m sfm_result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_sfm_with_colmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProcessed_Data/360deg_img/0000\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProcessed_Data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mProcessed_Data/3D_PointCloud/0000.ply\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSfM completed. Dense point cloud saved to\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense_point_cloud.ply\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m, in \u001b[0;36mrun_sfm_with_colmap\u001b[0;34m(image_dir, output_dir, ply_point_cloud)\u001b[0m\n\u001b[1;32m     12\u001b[0m pycolmap\u001b[38;5;241m.\u001b[39mmatch_sequential(database_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/database.db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Step 3: Run SfM pipeline\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mrecon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_reconstruction\u001b[49m(database_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/database.db\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     16\u001b[0m                             image_dir\u001b[38;5;241m=\u001b[39mimage_dir,\n\u001b[1;32m     17\u001b[0m                             output_dir\u001b[38;5;241m=\u001b[39moutput_dir)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Step 4: Optionally load your training point cloud\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ply_point_cloud:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'pycolmap.Reconstruction' object has no attribute 'sparse_reconstruction'"
     ]
    }
   ],
   "source": [
    "import pycolmap\n",
    "import os\n",
    "\n",
    "def run_sfm_with_colmap(image_dir, output_dir, ply_point_cloud=None):\n",
    "    # Initialize COLMAP's SfM\n",
    "    recon = pycolmap.Reconstruction()\n",
    "\n",
    "    # Step 1: Feature extraction\n",
    "    pycolmap.extract_features(image_path=image_dir, database_path=f'{output_dir}/database.db')\n",
    "\n",
    "    # Step 2: Feature matching\n",
    "    pycolmap.match_sequential(database_path=f'{output_dir}/database.db')\n",
    "\n",
    "    # Step 3: Run SfM pipeline\n",
    "    recon.sparse_reconstruction(database_path=f'{output_dir}/database.db', \n",
    "                                image_dir=image_dir,\n",
    "                                output_dir=output_dir)\n",
    "    \n",
    "    # Step 4: Optionally load your training point cloud\n",
    "    if ply_point_cloud:\n",
    "        recon.import_points(ply_point_cloud)\n",
    "\n",
    "    # Step 5: Export the final point cloud\n",
    "    recon.export_ply(os.path.join(output_dir, 'dense_point_cloud.ply'))\n",
    "\n",
    "    return recon\n",
    "\n",
    "# Set directories\n",
    "image_dir = \"Processed_Data/360deg_img\"\n",
    "output_dir = \"SfM_Output\"\n",
    "ply_point_cloud = \"path_to_your_point_cloud.ply\"  # Optional, if used\n",
    "\n",
    "# Run SfM pipeline\n",
    "sfm_result = run_sfm_with_colmap('Processed_Data/360deg_img/0000', 'Processed_Data', 'Processed_Data/3D_PointCloud/0000.ply')\n",
    "print(\"SfM completed. Dense point cloud saved to\", os.path.join(output_dir, 'dense_point_cloud.ply'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "class SimpleMVSNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleMVSNet, self).__init__()\n",
    "        # Simple CNN for feature extraction\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        # Depth prediction layer\n",
    "        self.depth_pred = nn.Conv2d(512, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        depth = self.depth_pred(x)\n",
    "        return depth\n",
    "\n",
    "# Loss function comparing predicted depth with ground truth point cloud\n",
    "def depth_loss(predicted_depth, ground_truth_depth):\n",
    "    return F.mse_loss(predicted_depth, ground_truth_depth)\n",
    "\n",
    "# Function to train the model using point cloud data\n",
    "def train_mvsnet(model, dataloader, point_cloud_dir, num_epochs=10, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch in dataloader:\n",
    "            images, ground_truth_depth = batch\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            predicted_depth = model(images)\n",
    "            \n",
    "            # Calculate loss using the provided point cloud\n",
    "            loss = depth_loss(predicted_depth, ground_truth_depth)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}')\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "# Function to load the point cloud and convert to depth map for training\n",
    "def load_point_cloud_to_depth_map(point_cloud_dir, image_shape):\n",
    "    # Load your point clouds stored in .ply format\n",
    "    point_cloud_files = [os.path.join(point_cloud_dir, f) for f in os.listdir(point_cloud_dir) if f.endswith('.ply')]\n",
    "    depth_maps = []\n",
    "\n",
    "    for pc_file in point_cloud_files:\n",
    "        point_cloud = o3d.io.read_point_cloud(pc_file)\n",
    "        points = np.asarray(point_cloud.points)\n",
    "        \n",
    "        # Placeholder for depth map creation from the point cloud\n",
    "        # You should project the 3D points onto 2D image space using camera calibration\n",
    "        depth_map = np.zeros(image_shape)\n",
    "        depth_maps.append(depth_map)\n",
    "\n",
    "    return torch.tensor(depth_maps, dtype=torch.float32)\n",
    "\n",
    "# Function to create dataloader for training\n",
    "def create_dataloader(image_dir, point_cloud_dir, batch_size=4):\n",
    "    # Load images and ground truth depth maps\n",
    "    images = load_images(image_dir)\n",
    "    ground_truth_depth_maps = load_point_cloud_to_depth_map(point_cloud_dir, images[0].shape)\n",
    "\n",
    "    # Create a dataset and dataloader\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(images, dtype=torch.float32), ground_truth_depth_maps)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
