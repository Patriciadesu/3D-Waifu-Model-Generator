{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bpy\n",
    "import trimesh\n",
    "import glob , os , shutil\n",
    "import torch\n",
    "import numpy as np\n",
    "from plyfile import PlyData\n",
    "import cv2\n",
    "import argparse\n",
    "import time\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run in blender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bpy\n",
    "import glob\n",
    "local_path = \"/Users/pataranansethpakdee/Documents/GitHub/\"\n",
    "vrm_filepath = local_path+\"3D-Waifu-Model-Generator/Datasets/3D_VRMModel/\"\n",
    "processed_filepath = local_path + \"3D-Waifu-Model-Generator/Datasets/3D_ProcessedModel/\"\n",
    "image_filepath = local_path + \"3D-Waifu-Model-Generator/Datasets/2D_Image/\"\n",
    "vrm_files = glob.glob(f'{vrm_filepath}*')\n",
    "\n",
    "def purge_orphans():\n",
    "    bpy.ops.outliner.orphans_purge(do_local_ids=True, do_linked_ids=True, do_recursive=True)\n",
    "\n",
    "def clean_scene():\n",
    "    scene = bpy.context.scene\n",
    "    bpy.data.scenes.new(\"Scene\")\n",
    "    bpy.data.scenes.remove(scene, do_unlink=True)\n",
    "    purge_orphans()\n",
    "    \n",
    "def set_new_camera():\n",
    "    scene = bpy.context.scene\n",
    "    cam_data = bpy.data.cameras.new(name=\"Camera\")\n",
    "    cam = bpy.data.objects.new(name=\"Camera\", object_data=cam_data)\n",
    "    scene.collection.objects.link(cam)\n",
    "    scene.camera = cam\n",
    "    cam.location = (0, -5.3, 0.8)\n",
    "    cam.rotation_euler = (1.5708, 0, 0) \n",
    "\n",
    "def new_scene():\n",
    "    clean_scene()\n",
    "    set_new_camera()\n",
    "\n",
    "count = 0\n",
    "\n",
    "for file in vrm_files:\n",
    "    \n",
    "    file_name = str(count).zfill(4)\n",
    "\n",
    "    clean_scene()\n",
    "    set_new_camera()\n",
    "    bpy.ops.import_scene.vrm(filepath=file)\n",
    "    scene = bpy.context.scene\n",
    "\n",
    "\n",
    "    # Render settings\n",
    "    scene.render.filepath = f'{image_filepath}{file_name}.png'\n",
    "    scene.render.image_settings.file_format = 'PNG'\n",
    "    scene.render.resolution_x = 1920\n",
    "    scene.render.resolution_y = 1080\n",
    "\n",
    "    # Render the image\n",
    "    bpy.ops.render.render(write_still=True)\n",
    "\n",
    "    #bpy.ops.export_scene.gltf(filepath=f\"{processed_filepath}{file_name}.glb\")\n",
    "    bpy.ops.export_scene.gltf(filepath=f\"{processed_filepath}{file_name}.glb\", \n",
    "                         export_format='GLB', \n",
    "                         export_image_format='JPEG',\n",
    "                         export_image_add_webp=True,\n",
    "                         export_image_webp_fallback=True,\n",
    "                         export_texcoords=True,\n",
    "                         export_normals=True,\n",
    "                         export_materials='EXPORT',\n",
    "                         export_vertex_color='MATERIAL',\n",
    "                         export_all_vertex_colors=True)\n",
    "    count+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert glb to pointcloud (ply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this in Window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] Write PLY failed: unable to open file: Processed_Data/3D_PointCloud/3D_ProcessedModel\\0000.ply\n",
      "[Open3D WARNING] Write PLY failed: unable to open file: Processed_Data/3D_PointCloud/3D_ProcessedModel\\0001.ply\n"
     ]
    }
   ],
   "source": [
    "#Convert .ply to point cloud\n",
    "ply_path = \"Datasets/3D_ProcessedModel\"\n",
    "ply_files = glob.glob(r\"Datasets/3D_ProcessedModel/*\")\n",
    "\n",
    "for file in ply_files:\n",
    "    file_name = (file.split('/')[-1]).split(\".\")[0]\n",
    "    path = f\"{ply_path}/{file_name}.glb\"\n",
    "    scene = trimesh.load(file)\n",
    "\n",
    "    # Traverse all geometries (meshes) in the scene\n",
    "    point_clouds = []\n",
    "    for name, mesh in scene.geometry.items():\n",
    "        vertices = mesh.vertices\n",
    "        colors = mesh.visual.to_color().vertex_colors  # Optional: may be texture-based\n",
    "        \n",
    "        # Sample points from the mesh\n",
    "        num_samples = 5000  # Adjust this number for more points\n",
    "        if len(vertices) > num_samples:\n",
    "            indices = np.random.choice(len(vertices), num_samples, replace=False)\n",
    "            sampled_vertices = vertices[indices]\n",
    "            sampled_colors = colors[indices]\n",
    "        else:\n",
    "            sampled_vertices = vertices\n",
    "            sampled_colors = colors\n",
    "\n",
    "        # Combine vertices and colors into a point cloud\n",
    "        point_cloud_with_color = [(*v, *c[:3]) for v, c in zip(sampled_vertices, sampled_colors)]\n",
    "        point_clouds.append(point_cloud_with_color)\n",
    "    all_points = [point for pc in point_clouds for point in pc]\n",
    "    points = np.array([p[:3] for p in all_points])\n",
    "    colors = np.array([p[3:] for p in all_points]) / 255.0  # Normalize color values\n",
    "\n",
    "    # Create Open3D point cloud\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    o3d.io.write_point_cloud(f\"Processed_Data/3D_PointCloud/{file_name}.ply\", pcd)\n",
    "#o3d.io.write_point_cloud(f\"Processed_Data/3D_PointCloud/{file_name}.pts\",pcd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = glob.glob(\"Processed_Data/3D_PointCloud/*\")\n",
    "img_path = \"Processed_Data/360deg_img/\"\n",
    "comfy_path = \"C:/Users/patar/Downloads/ComfyUI_windows_portable_nvidia/ComfyUI_windows_portable/ComfyUI/output\"\n",
    "names = []\n",
    "for file_name in file_names:\n",
    "    folder_name = file_name.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    names.append(folder_name)\n",
    "    if(not os.path.exists(img_path+folder_name)):\n",
    "        os.mkdir(img_path+folder_name)\n",
    "comfy_files = glob.glob(comfy_path+\"/*\")\n",
    "for name in names:\n",
    "    for comfy_file in comfy_files:\n",
    "        if(name==comfy_file.split(\"\\\\\")[-1].split('.')[0]):\n",
    "            shutil.move(comfy_file,img_path+name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction (SuperPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_images(image_paths):\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        img = cv2.imread(path)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'SuperPointPretrainedNetwork'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/magicleap/SuperPointPretrainedNetwork\n",
    "!cd SuperPointPretrainedNetwork\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperPointNet(torch.nn.Module):\n",
    "  \"\"\" Pytorch definition of SuperPoint Network. \"\"\"\n",
    "  def __init__(self):\n",
    "    super(SuperPointNet, self).__init__()\n",
    "    self.relu = torch.nn.ReLU(inplace=True)\n",
    "    self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "    c1, c2, c3, c4, c5, d1 = 64, 64, 128, 128, 256, 256\n",
    "    # Shared Encoder.\n",
    "    self.conv1a = torch.nn.Conv2d(1, c1, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv1b = torch.nn.Conv2d(c1, c1, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2a = torch.nn.Conv2d(c1, c2, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv2b = torch.nn.Conv2d(c2, c2, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3a = torch.nn.Conv2d(c2, c3, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv3b = torch.nn.Conv2d(c3, c3, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv4a = torch.nn.Conv2d(c3, c4, kernel_size=3, stride=1, padding=1)\n",
    "    self.conv4b = torch.nn.Conv2d(c4, c4, kernel_size=3, stride=1, padding=1)\n",
    "    # Detector Head.\n",
    "    self.convPa = torch.nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
    "    self.convPb = torch.nn.Conv2d(c5, 65, kernel_size=1, stride=1, padding=0)\n",
    "    # Descriptor Head.\n",
    "    self.convDa = torch.nn.Conv2d(c4, c5, kernel_size=3, stride=1, padding=1)\n",
    "    self.convDb = torch.nn.Conv2d(c5, d1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\" Forward pass that jointly computes unprocessed point and descriptor\n",
    "    tensors.\n",
    "    Input\n",
    "      x: Image pytorch tensor shaped N x 1 x H x W.\n",
    "    Output\n",
    "      semi: Output point pytorch tensor shaped N x 65 x H/8 x W/8.\n",
    "      desc: Output descriptor pytorch tensor shaped N x 256 x H/8 x W/8.\n",
    "    \"\"\"\n",
    "    # Shared Encoder.\n",
    "    x = self.relu(self.conv1a(x))\n",
    "    x = self.relu(self.conv1b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv2a(x))\n",
    "    x = self.relu(self.conv2b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv3a(x))\n",
    "    x = self.relu(self.conv3b(x))\n",
    "    x = self.pool(x)\n",
    "    x = self.relu(self.conv4a(x))\n",
    "    x = self.relu(self.conv4b(x))\n",
    "    # Detector Head.\n",
    "    cPa = self.relu(self.convPa(x))\n",
    "    semi = self.convPb(cPa)\n",
    "    # Descriptor Head.\n",
    "    cDa = self.relu(self.convDa(x))\n",
    "    desc = self.convDb(cDa)\n",
    "    dn = torch.norm(desc, p=2, dim=1) # Compute the norm.\n",
    "    desc = desc.div(torch.unsqueeze(dn, 1)) # Divide by norm to normalize.\n",
    "    return semi, desc\n",
    "class SuperPointFrontend(object):\n",
    "  \"\"\" Wrapper around pytorch net to help with pre and post image processing. \"\"\"\n",
    "  def __init__(self, weights_path, nms_dist, conf_thresh, nn_thresh,\n",
    "               cuda=False):\n",
    "    self.name = 'SuperPoint'\n",
    "    self.cuda = cuda\n",
    "    self.nms_dist = nms_dist\n",
    "    self.conf_thresh = conf_thresh\n",
    "    self.nn_thresh = nn_thresh # L2 descriptor distance for good match.\n",
    "    self.cell = 8 # Size of each output cell. Keep this fixed.\n",
    "    self.border_remove = 4 # Remove points this close to the border.\n",
    "\n",
    "    # Load the network in inference mode.\n",
    "    self.net = SuperPointNet()\n",
    "    if cuda:\n",
    "      # Train on GPU, deploy on GPU.\n",
    "      self.net.load_state_dict(torch.load(weights_path))\n",
    "      self.net = self.net.cuda()\n",
    "    else:\n",
    "      # Train on GPU, deploy on CPU.\n",
    "      self.net.load_state_dict(torch.load(weights_path,\n",
    "                               map_location=lambda storage, loc: storage))\n",
    "    self.net.eval()\n",
    "\n",
    "  def nms_fast(self, in_corners, H, W, dist_thresh):\n",
    "   \n",
    "    grid = np.zeros((H, W)).astype(int) # Track NMS data.\n",
    "    inds = np.zeros((H, W)).astype(int) # Store indices of points.\n",
    "    # Sort by confidence and round to nearest int.\n",
    "    inds1 = np.argsort(-in_corners[2,:])\n",
    "    corners = in_corners[:,inds1]\n",
    "    rcorners = corners[:2,:].round().astype(int) # Rounded corners.\n",
    "    # Check for edge case of 0 or 1 corners.\n",
    "    if rcorners.shape[1] == 0:\n",
    "      return np.zeros((3,0)).astype(int), np.zeros(0).astype(int)\n",
    "    if rcorners.shape[1] == 1:\n",
    "      out = np.vstack((rcorners, in_corners[2])).reshape(3,1)\n",
    "      return out, np.zeros((1)).astype(int)\n",
    "    # Initialize the grid.\n",
    "    for i, rc in enumerate(rcorners.T):\n",
    "      grid[rcorners[1,i], rcorners[0,i]] = 1\n",
    "      inds[rcorners[1,i], rcorners[0,i]] = i\n",
    "    # Pad the border of the grid, so that we can NMS points near the border.\n",
    "    pad = dist_thresh\n",
    "    grid = np.pad(grid, ((pad,pad), (pad,pad)), mode='constant')\n",
    "    # Iterate through points, highest to lowest conf, suppress neighborhood.\n",
    "    count = 0\n",
    "    for i, rc in enumerate(rcorners.T):\n",
    "      # Account for top and left padding.\n",
    "      pt = (rc[0]+pad, rc[1]+pad)\n",
    "      if grid[pt[1], pt[0]] == 1: # If not yet suppressed.\n",
    "        grid[pt[1]-pad:pt[1]+pad+1, pt[0]-pad:pt[0]+pad+1] = 0\n",
    "        grid[pt[1], pt[0]] = -1\n",
    "        count += 1\n",
    "    # Get all surviving -1's and return sorted array of remaining corners.\n",
    "    keepy, keepx = np.where(grid==-1)\n",
    "    keepy, keepx = keepy - pad, keepx - pad\n",
    "    inds_keep = inds[keepy, keepx]\n",
    "    out = corners[:, inds_keep]\n",
    "    values = out[-1, :]\n",
    "    inds2 = np.argsort(-values)\n",
    "    out = out[:, inds2]\n",
    "    out_inds = inds1[inds_keep[inds2]]\n",
    "    return out, out_inds\n",
    "\n",
    "  def run(self, img):\n",
    "    assert img.ndim == 2, 'Image must be grayscale.'\n",
    "    assert img.dtype == np.float32, 'Image must be float32.'\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    inp = img.copy()\n",
    "    inp = (inp.reshape(1, H, W))\n",
    "    inp = torch.from_numpy(inp)\n",
    "    inp = torch.autograd.Variable(inp).view(1, 1, H, W)\n",
    "    if self.cuda:\n",
    "      inp = inp.cuda()\n",
    "    # Forward pass of network.\n",
    "    outs = self.net.forward(inp)\n",
    "    semi, coarse_desc = outs[0], outs[1]\n",
    "    # Convert pytorch -> numpy.\n",
    "    semi = semi.data.cpu().numpy().squeeze()\n",
    "    # --- Process points.\n",
    "    dense = np.exp(semi) # Softmax.\n",
    "    dense = dense / (np.sum(dense, axis=0)+.00001) # Should sum to 1.\n",
    "    # Remove dustbin.\n",
    "    nodust = dense[:-1, :, :]\n",
    "    # Reshape to get full resolution heatmap.\n",
    "    Hc = int(H / self.cell)\n",
    "    Wc = int(W / self.cell)\n",
    "    nodust = nodust.transpose(1, 2, 0)\n",
    "    heatmap = np.reshape(nodust, [Hc, Wc, self.cell, self.cell])\n",
    "    heatmap = np.transpose(heatmap, [0, 2, 1, 3])\n",
    "    heatmap = np.reshape(heatmap, [Hc*self.cell, Wc*self.cell])\n",
    "    xs, ys = np.where(heatmap >= self.conf_thresh) # Confidence threshold.\n",
    "    if len(xs) == 0:\n",
    "      return np.zeros((3, 0)), None, None\n",
    "    pts = np.zeros((3, len(xs))) # Populate point data sized 3xN.\n",
    "    pts[0, :] = ys\n",
    "    pts[1, :] = xs\n",
    "    pts[2, :] = heatmap[xs, ys]\n",
    "    pts, _ = self.nms_fast(pts, H, W, dist_thresh=self.nms_dist) # Apply NMS.\n",
    "    inds = np.argsort(pts[2,:])\n",
    "    pts = pts[:,inds[::-1]] # Sort by confidence.\n",
    "    # Remove points along border.\n",
    "    bord = self.border_remove\n",
    "    toremoveW = np.logical_or(pts[0, :] < bord, pts[0, :] >= (W-bord))\n",
    "    toremoveH = np.logical_or(pts[1, :] < bord, pts[1, :] >= (H-bord))\n",
    "    toremove = np.logical_or(toremoveW, toremoveH)\n",
    "    pts = pts[:, ~toremove]\n",
    "    # --- Process descriptor.\n",
    "    D = coarse_desc.shape[1]\n",
    "    if pts.shape[1] == 0:\n",
    "      desc = np.zeros((D, 0))\n",
    "    else:\n",
    "      # Interpolate into descriptor map using 2D point locations.\n",
    "      samp_pts = torch.from_numpy(pts[:2, :].copy())\n",
    "      samp_pts[0, :] = (samp_pts[0, :] / (float(W)/2.)) - 1.\n",
    "      samp_pts[1, :] = (samp_pts[1, :] / (float(H)/2.)) - 1.\n",
    "      samp_pts = samp_pts.transpose(0, 1).contiguous()\n",
    "      samp_pts = samp_pts.view(1, 1, -1, 2)\n",
    "      samp_pts = samp_pts.float()\n",
    "      if self.cuda:\n",
    "        samp_pts = samp_pts.cuda()\n",
    "      desc = torch.nn.functional.grid_sample(coarse_desc, samp_pts)\n",
    "      desc = desc.data.cpu().numpy().reshape(D, -1)\n",
    "      desc /= np.linalg.norm(desc, axis=0)[np.newaxis, :]\n",
    "    return pts, desc, heatmap\n",
    "class PointTracker(object):\n",
    "  \"\"\" Class to manage a fixed memory of points and descriptors that enables\n",
    "  sparse optical flow point tracking.\n",
    "\n",
    "  Internally, the tracker stores a 'tracks' matrix sized M x (2+L), of M\n",
    "  tracks with maximum length L, where each row corresponds to:\n",
    "  row_m = [track_id_m, avg_desc_score_m, point_id_0_m, ..., point_id_L-1_m].\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, max_length, nn_thresh):\n",
    "    if max_length < 2:\n",
    "      raise ValueError('max_length must be greater than or equal to 2.')\n",
    "    self.maxl = max_length\n",
    "    self.nn_thresh = nn_thresh\n",
    "    self.all_pts = []\n",
    "    for n in range(self.maxl):\n",
    "      self.all_pts.append(np.zeros((2, 0)))\n",
    "    self.last_desc = None\n",
    "    self.tracks = np.zeros((0, self.maxl+2))\n",
    "    self.track_count = 0\n",
    "    self.max_score = 9999\n",
    "\n",
    "  def nn_match_two_way(self, desc1, desc2, nn_thresh):\n",
    "    \"\"\"\n",
    "    Performs two-way nearest neighbor matching of two sets of descriptors, such\n",
    "    that the NN match from descriptor A->B must equal the NN match from B->A.\n",
    "\n",
    "    Inputs:\n",
    "      desc1 - NxM numpy matrix of N corresponding M-dimensional descriptors.\n",
    "      desc2 - NxM numpy matrix of N corresponding M-dimensional descriptors.\n",
    "      nn_thresh - Optional descriptor distance below which is a good match.\n",
    "\n",
    "    Returns:\n",
    "      matches - 3xL numpy array, of L matches, where L <= N and each column i is\n",
    "                a match of two descriptors, d_i in image 1 and d_j' in image 2:\n",
    "                [d_i index, d_j' index, match_score]^T\n",
    "    \"\"\"\n",
    "    assert desc1.shape[0] == desc2.shape[0]\n",
    "    if desc1.shape[1] == 0 or desc2.shape[1] == 0:\n",
    "      return np.zeros((3, 0))\n",
    "    if nn_thresh < 0.0:\n",
    "      raise ValueError('\\'nn_thresh\\' should be non-negative')\n",
    "    # Compute L2 distance. Easy since vectors are unit normalized.\n",
    "    dmat = np.dot(desc1.T, desc2)\n",
    "    dmat = np.sqrt(2-2*np.clip(dmat, -1, 1))\n",
    "    # Get NN indices and scores.\n",
    "    idx = np.argmin(dmat, axis=1)\n",
    "    scores = dmat[np.arange(dmat.shape[0]), idx]\n",
    "    # Threshold the NN matches.\n",
    "    keep = scores < nn_thresh\n",
    "    # Check if nearest neighbor goes both directions and keep those.\n",
    "    idx2 = np.argmin(dmat, axis=0)\n",
    "    keep_bi = np.arange(len(idx)) == idx2[idx]\n",
    "    keep = np.logical_and(keep, keep_bi)\n",
    "    idx = idx[keep]\n",
    "    scores = scores[keep]\n",
    "    # Get the surviving point indices.\n",
    "    m_idx1 = np.arange(desc1.shape[1])[keep]\n",
    "    m_idx2 = idx\n",
    "    # Populate the final 3xN match data structure.\n",
    "    matches = np.zeros((3, int(keep.sum())))\n",
    "    matches[0, :] = m_idx1\n",
    "    matches[1, :] = m_idx2\n",
    "    matches[2, :] = scores\n",
    "    return matches\n",
    "\n",
    "  def get_offsets(self):\n",
    "    \"\"\" Iterate through list of points and accumulate an offset value. Used to\n",
    "    index the global point IDs into the list of points.\n",
    "\n",
    "    Returns\n",
    "      offsets - N length array with integer offset locations.\n",
    "    \"\"\"\n",
    "    # Compute id offsets.\n",
    "    offsets = []\n",
    "    offsets.append(0)\n",
    "    for i in range(len(self.all_pts)-1): # Skip last camera size, not needed.\n",
    "      offsets.append(self.all_pts[i].shape[1])\n",
    "    offsets = np.array(offsets)\n",
    "    offsets = np.cumsum(offsets)\n",
    "    return offsets\n",
    "\n",
    "  def update(self, pts, desc):\n",
    "    \"\"\" Add a new set of point and descriptor observations to the tracker.\n",
    "\n",
    "    Inputs\n",
    "      pts - 3xN numpy array of 2D point observations.\n",
    "      desc - DxN numpy array of corresponding D dimensional descriptors.\n",
    "    \"\"\"\n",
    "    if pts is None or desc is None:\n",
    "      print('PointTracker: Warning, no points were added to tracker.')\n",
    "      return\n",
    "    assert pts.shape[1] == desc.shape[1]\n",
    "    # Initialize last_desc.\n",
    "    if self.last_desc is None:\n",
    "      self.last_desc = np.zeros((desc.shape[0], 0))\n",
    "    # Remove oldest points, store its size to update ids later.\n",
    "    remove_size = self.all_pts[0].shape[1]\n",
    "    self.all_pts.pop(0)\n",
    "    self.all_pts.append(pts)\n",
    "    # Remove oldest point in track.\n",
    "    self.tracks = np.delete(self.tracks, 2, axis=1)\n",
    "    # Update track offsets.\n",
    "    for i in range(2, self.tracks.shape[1]):\n",
    "      self.tracks[:, i] -= remove_size\n",
    "    self.tracks[:, 2:][self.tracks[:, 2:] < -1] = -1\n",
    "    offsets = self.get_offsets()\n",
    "    # Add a new -1 column.\n",
    "    self.tracks = np.hstack((self.tracks, -1*np.ones((self.tracks.shape[0], 1))))\n",
    "    # Try to append to existing tracks.\n",
    "    matched = np.zeros((pts.shape[1])).astype(bool)\n",
    "    matches = self.nn_match_two_way(self.last_desc, desc, self.nn_thresh)\n",
    "    for match in matches.T:\n",
    "      # Add a new point to it's matched track.\n",
    "      id1 = int(match[0]) + offsets[-2]\n",
    "      id2 = int(match[1]) + offsets[-1]\n",
    "      found = np.argwhere(self.tracks[:, -2] == id1)\n",
    "      if found.shape[0] > 0:\n",
    "        matched[int(match[1])] = True\n",
    "        row = int(found)\n",
    "        self.tracks[row, -1] = id2\n",
    "        if self.tracks[row, 1] == self.max_score:\n",
    "          # Initialize track score.\n",
    "          self.tracks[row, 1] = match[2]\n",
    "        else:\n",
    "          # Update track score with running average.\n",
    "          # NOTE(dd): this running average can contain scores from old matches\n",
    "          #           not contained in last max_length track points.\n",
    "          track_len = (self.tracks[row, 2:] != -1).sum() - 1.\n",
    "          frac = 1. / float(track_len)\n",
    "          self.tracks[row, 1] = (1.-frac)*self.tracks[row, 1] + frac*match[2]\n",
    "    # Add unmatched tracks.\n",
    "    new_ids = np.arange(pts.shape[1]) + offsets[-1]\n",
    "    new_ids = new_ids[~matched]\n",
    "    new_tracks = -1*np.ones((new_ids.shape[0], self.maxl + 2))\n",
    "    new_tracks[:, -1] = new_ids\n",
    "    new_num = new_ids.shape[0]\n",
    "    new_trackids = self.track_count + np.arange(new_num)\n",
    "    new_tracks[:, 0] = new_trackids\n",
    "    new_tracks[:, 1] = self.max_score*np.ones(new_ids.shape[0])\n",
    "    self.tracks = np.vstack((self.tracks, new_tracks))\n",
    "    self.track_count += new_num # Update the track count.\n",
    "    # Remove empty tracks.\n",
    "    keep_rows = np.any(self.tracks[:, 2:] >= 0, axis=1)\n",
    "    self.tracks = self.tracks[keep_rows, :]\n",
    "    # Store the last descriptors.\n",
    "    self.last_desc = desc.copy()\n",
    "    return\n",
    "\n",
    "  def get_tracks(self, min_length):\n",
    "    \"\"\" Retrieve point tracks of a given minimum length.\n",
    "    Input\n",
    "      min_length - integer >= 1 with minimum track length\n",
    "    Output\n",
    "      returned_tracks - M x (2+L) sized matrix storing track indices, where\n",
    "        M is the number of tracks and L is the maximum track length.\n",
    "    \"\"\"\n",
    "    if min_length < 1:\n",
    "      raise ValueError('\\'min_length\\' too small.')\n",
    "    valid = np.ones((self.tracks.shape[0])).astype(bool)\n",
    "    good_len = np.sum(self.tracks[:, 2:] != -1, axis=1) >= min_length\n",
    "    # Remove tracks which do not have an observation in most recent frame.\n",
    "    not_headless = (self.tracks[:, -1] != -1)\n",
    "    keepers = np.logical_and.reduce((valid, good_len, not_headless))\n",
    "    returned_tracks = self.tracks[keepers, :].copy()\n",
    "    return returned_tracks\n",
    "\n",
    "  def draw_tracks(self, out, tracks):\n",
    "    \"\"\" Visualize tracks all overlayed on a single image.\n",
    "    Inputs\n",
    "      out - numpy uint8 image sized HxWx3 upon which tracks are overlayed.\n",
    "      tracks - M x (2+L) sized matrix storing track info.\n",
    "    \"\"\"\n",
    "    # Store the number of points per camera.\n",
    "    pts_mem = self.all_pts\n",
    "    N = len(pts_mem) # Number of cameras/images.\n",
    "    # Get offset ids needed to reference into pts_mem.\n",
    "    offsets = self.get_offsets()\n",
    "    # Width of track and point circles to be drawn.\n",
    "    stroke = 1\n",
    "    # Iterate through each track and draw it.\n",
    "    for track in tracks:\n",
    "      clr = myjet[int(np.clip(np.floor(track[1]*10), 0, 9)), :]*255\n",
    "      for i in range(N-1):\n",
    "        if track[i+2] == -1 or track[i+3] == -1:\n",
    "          continue\n",
    "        offset1 = offsets[i]\n",
    "        offset2 = offsets[i+1]\n",
    "        idx1 = int(track[i+2]-offset1)\n",
    "        idx2 = int(track[i+3]-offset2)\n",
    "        pt1 = pts_mem[i][:2, idx1]\n",
    "        pt2 = pts_mem[i+1][:2, idx2]\n",
    "        p1 = (int(round(pt1[0])), int(round(pt1[1])))\n",
    "        p2 = (int(round(pt2[0])), int(round(pt2[1])))\n",
    "        cv2.line(out, p1, p2, clr, thickness=stroke, lineType=16)\n",
    "        # Draw end points of each track.\n",
    "        if i == N-2:\n",
    "          clr2 = (255, 0, 0)\n",
    "          cv2.circle(out, p2, stroke, clr2, -1, lineType=16)\n",
    "class VideoStreamer(object):\n",
    "  \"\"\" Class to help process image streams. Three types of possible inputs:\"\n",
    "    1.) USB Webcam.\n",
    "    2.) A directory of images (files in directory matching 'img_glob').\n",
    "    3.) A video file, such as an .mp4 or .avi file.\n",
    "  \"\"\"\n",
    "  def __init__(self, basedir, height, width):\n",
    "    self.cap = []\n",
    "    self.camera = False\n",
    "    self.listing = []\n",
    "    self.sizer = [height, width]\n",
    "    self.i = 0\n",
    "\n",
    "    self.maxlen = 1000000\n",
    "    search = os.path.join(basedir, '*')\n",
    "    self.listing = glob.glob(search)\n",
    "    self.listing.sort()\n",
    "    self.listing = self.listing[::1]\n",
    "    self.maxlen = len(self.listing)\n",
    "    if self.maxlen == 0:\n",
    "      raise IOError('No images were found (maybe bad \\'--img_glob\\' parameter?)')\n",
    "\n",
    "  def read_image(self, impath, img_size):\n",
    "    grayim = cv2.imread(impath, 0)\n",
    "    if grayim is None:\n",
    "      raise Exception('Error reading image %s' % impath)\n",
    "    # Image is resized via opencv.\n",
    "    interp = cv2.INTER_AREA\n",
    "    grayim = cv2.resize(grayim, (img_size[1], img_size[0]), interpolation=interp)\n",
    "    grayim = (grayim.astype('float32') / 255.)\n",
    "    return grayim\n",
    "\n",
    "  def next_frame(self):\n",
    "    \"\"\" Return the next frame, and increment internal counter.\n",
    "    Returns\n",
    "       image: Next H x W image.\n",
    "       status: True or False depending whether image was loaded.\n",
    "    \"\"\"\n",
    "    if self.i == self.maxlen:\n",
    "      return (None, False)\n",
    "    if self.camera:\n",
    "      ret, input_image = self.cap.read()\n",
    "      if ret is False:\n",
    "        print('VideoStreamer: Cannot get image from camera (maybe bad --camid?)')\n",
    "        return (None, False)\n",
    "      if self.video_file:\n",
    "        self.cap.set(cv2.CAP_PROP_POS_FRAMES, self.listing[self.i])\n",
    "      input_image = cv2.resize(input_image, (self.sizer[1], self.sizer[0]),\n",
    "                               interpolation=cv2.INTER_AREA)\n",
    "      input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2GRAY)\n",
    "      input_image = input_image.astype('float')/255.0\n",
    "    else:\n",
    "      image_file = self.listing[self.i]\n",
    "      input_image = self.read_image(image_file, self.sizer)\n",
    "    # Increment internal counter.\n",
    "    self.i = self.i + 1\n",
    "    input_image = input_image.astype('float32')\n",
    "    return (input_image, True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # Parse command line arguments.\n",
    "myjet = np.array([[0.        , 0.        , 0.5       ],\n",
    "                  [0.        , 0.        , 0.99910873],\n",
    "                  [0.        , 0.37843137, 1.        ],\n",
    "                  [0.        , 0.83333333, 1.        ],\n",
    "                  [0.30044276, 1.        , 0.66729918],\n",
    "                  [0.66729918, 1.        , 0.30044276],\n",
    "                  [1.        , 0.90123457, 0.        ],\n",
    "                  [1.        , 0.48002905, 0.        ],\n",
    "                  [0.99910873, 0.07334786, 0.        ],\n",
    "                  [0.5       , 0.        , 0.        ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patar\\AppData\\Local\\Temp\\ipykernel_20792\\1850739445.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(weights_path,\n"
     ]
    }
   ],
   "source": [
    "input_path = 'Processed_Data/360deg_img'\n",
    "output_path = 'Processed_Data/Feature Map'\n",
    "input_dirs = glob.glob(input_path+'/*')\n",
    "img_w = 512\n",
    "img_h = 512\n",
    "weight_path = 'SuperPointPretrainedNetwork/superpoint_v1.pth'\n",
    "dist = 4\n",
    "conf_thresh=0.015\n",
    "nn_thresh = 0.7\n",
    "GPU = False\n",
    "fe = SuperPointFrontend(weights_path=weight_path,\n",
    "                        nms_dist=dist,\n",
    "                        conf_thresh=conf_thresh,\n",
    "                        nn_thresh=nn_thresh,\n",
    "                        cuda=GPU)\n",
    "tracker = PointTracker(5, nn_thresh=fe.nn_thresh)\n",
    "\n",
    "def ExtractFeature(input_dirs,output_dirs):\n",
    "    vs = VideoStreamer(input_dirs, 512, 512)\n",
    "\n",
    "    win = 'SuperPoint Tracker'\n",
    "    # Font parameters for visualizaton.\n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    font_clr = (255, 255, 255)\n",
    "    font_pt = (4, 12)\n",
    "    font_sc = 0.4\n",
    "\n",
    "    if not os.path.exists(output_dirs):\n",
    "        os.makedirs(output_dirs)\n",
    "    while True:\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Get a new image.\n",
    "        img, status = vs.next_frame()\n",
    "        if status is False:\n",
    "            break\n",
    "\n",
    "        # Get points and descriptors.\n",
    "        pts, desc, heatmap = fe.run(img)\n",
    "\n",
    "        # Add points and descriptors to the tracker.\n",
    "        tracker.update(pts, desc)\n",
    "\n",
    "        # Get tracks for points which were match successfully across all frames.\n",
    "        tracks = tracker.get_tracks(2)\n",
    "\n",
    "        # Primary output - Show point tracks overlayed on top of input image.\n",
    "        out1 = (np.dstack((img, img, img)) * 255.).astype('uint8')\n",
    "        tracks[:, 1] /= float(fe.nn_thresh) # Normalize track scores to [0,1].\n",
    "        tracker.draw_tracks(out1, tracks)\n",
    "        if True:\n",
    "            cv2.putText(out1, 'Point Tracks', font_pt, font, font_sc, font_clr, lineType=16)\n",
    "\n",
    "        # Extra output -- Show current point detections.\n",
    "        out2 = (np.dstack((img, img, img)) * 255.).astype('uint8')\n",
    "        for pt in pts.T:\n",
    "            pt1 = (int(round(pt[0])), int(round(pt[1])))\n",
    "            cv2.circle(out2, pt1, 1, (0, 255, 0), -1, lineType=16)\n",
    "        cv2.putText(out2, 'Raw Point Detections', font_pt, font, font_sc, font_clr, lineType=16)\n",
    "\n",
    "        # Extra output -- Show the point confidence heatmap.\n",
    "        if heatmap is not None:\n",
    "            min_conf = 0.001\n",
    "            heatmap[heatmap < min_conf] = min_conf\n",
    "            heatmap = -np.log(heatmap)\n",
    "            heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + .00001)\n",
    "            out3 = myjet[np.round(np.clip(heatmap*10, 0, 9)).astype('int'), :]\n",
    "            out3 = (out3*255).astype('uint8')\n",
    "        else:\n",
    "            out3 = np.zeros_like(out2)\n",
    "        cv2.putText(out3, 'Raw Point Confidences', font_pt, font, font_sc, font_clr, lineType=16)\n",
    "\n",
    "        # Resize final output.\n",
    "        if False:\n",
    "            out = np.hstack((out1, out2, out3))\n",
    "            out = cv2.resize(out, (3*2*512, 2*512))\n",
    "        else:\n",
    "            out = cv2.resize(out2, (2*512, 2*512))\n",
    "\n",
    "        # Optionally write images to disk.\n",
    "        if True:\n",
    "            out_file = os.path.join(output_dirs, 'frame_%05d.png' % vs.i)\n",
    "            cv2.imwrite(out_file, out)\n",
    "\n",
    "\n",
    "    print(f'==> Finshed {output_dirs}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:4373: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "C:\\Users\\patar\\AppData\\Local\\Temp\\ipykernel_20792\\1850739445.py:299: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  row = int(found)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Finshed Processed_Data/Feature Map\\360deg_img\\0000.\n",
      "==> Finshed Processed_Data/Feature Map\\360deg_img\\0001.\n"
     ]
    }
   ],
   "source": [
    "for dirPath in input_dirs:\n",
    "    folderName = dirPath.split(\"/\")[-1]\n",
    "    outputPath = os.path.join(output_path,folderName)\n",
    "    ExtractFeature(dirPath,outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patar\\AppData\\Local\\Temp\\ipykernel_20792\\1850739445.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.net.load_state_dict(torch.load(weights_path,\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def match_images_across_multiple(img_folder,outputFolder, fe, nn_thresh=0.7, save_matches=False):\n",
    "    # Get all image paths from the folder\n",
    "    img_paths = sorted(glob.glob((img_folder+'/*')))\n",
    "    num_images = len(img_paths)\n",
    "    assert num_images >= 2, \"Need at least two images for matching.\"\n",
    "\n",
    "    all_matches = []  # To store matches across all images\n",
    "    if not os.path.exists(outputFolder):\n",
    "        os.makedirs(outputFolder)\n",
    "    # Iterate over image pairs (i, i+1)\n",
    "    for i in range(num_images - 1):\n",
    "\n",
    "        # Read consecutive images\n",
    "        img1 = cv2.imread(img_paths[i], cv2.IMREAD_GRAYSCALE)\n",
    "        img2 = cv2.imread(img_paths[i+1], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img1 = img1.astype(np.float32) / 255.0\n",
    "        img2 = img2.astype(np.float32) / 255.0\n",
    "\n",
    "        # Extract keypoints and descriptors using SuperPoint\n",
    "        pts1, desc1, _ = fe.run(img1)\n",
    "        pts2, desc2, _ = fe.run(img2)\n",
    "\n",
    "        # Match descriptors between img1 and img2\n",
    "        good_matches = feature_match(desc1, desc2, nn_thresh)\n",
    "        \n",
    "        # Visualize matches if desired\n",
    "        if save_matches:\n",
    "            visualize_and_save(img1, img2, pts1, pts2, good_matches, i,outputFolder)\n",
    "        match_data = [(m.queryIdx, m.trainIdx, m.distance) for m in good_matches]\n",
    "\n",
    "        # Save matches for this image pair as a .npy file\n",
    "        match_file = os.path.join(outputFolder, f'matches_{i}_{i + 1}.npy')\n",
    "        np.save(match_file, match_data)\n",
    "\n",
    "        all_matches.append(good_matches)\n",
    "\n",
    "    return all_matches\n",
    "\n",
    "def feature_match(desc1, desc2, nn_thresh=0.7):\n",
    "    \"\"\"Performs nearest neighbor matching between two descriptors.\"\"\"\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "    desc1 = desc1.T\n",
    "    desc2 = desc2.T\n",
    "    matches = bf.match(desc1, desc2)\n",
    "    \n",
    "    # Filter good matches\n",
    "    good_matches = [m for m in matches if m.distance < nn_thresh]\n",
    "    return good_matches\n",
    "\n",
    "def visualize_and_save(img1, img2, pts1, pts2, matches, img_idx,outputFolder):\n",
    "    \"\"\"\n",
    "    Visualize and save the matches between two images.\n",
    "\n",
    "    Args:\n",
    "        img1: First image (float32 grayscale).\n",
    "        img2: Second image (float32 grayscale).\n",
    "        pts1: Keypoints from the first image.\n",
    "        pts2: Keypoints from the second image.\n",
    "        matches: Good matches between descriptors of the two images.\n",
    "        img_idx: Index of the image pair being processed.\n",
    "    \"\"\"\n",
    "    # Convert images back to uint8 for visualization\n",
    "    img1_u8 = (img1 * 255).astype(np.uint8)\n",
    "    img2_u8 = (img2 * 255).astype(np.uint8)\n",
    "\n",
    "    # Convert keypoints for visualization\n",
    "    kp1 = [cv2.KeyPoint(pt[0], pt[1], 1) for pt in pts1.T]\n",
    "    kp2 = [cv2.KeyPoint(pt[0], pt[1], 1) for pt in pts2.T]\n",
    "\n",
    "    # Draw matches\n",
    "    out_img = cv2.drawMatches(img1_u8, kp1, img2_u8, kp2, matches, None)\n",
    "\n",
    "    # Save output image\n",
    "    out_file = f'{outputFolder}/matches_{img_idx}_{img_idx+1}.png'\n",
    "    cv2.imwrite(out_file, out_img)\n",
    "\n",
    "\n",
    "# Define parameters and run matching across 21 images\n",
    "img_folder = glob.glob('Processed_Data/360deg_img/*')\n",
    "weight_path = 'SuperPointPretrainedNetwork/superpoint_v1.pth'\n",
    "dist = 4\n",
    "conf_thresh = 0.015\n",
    "nn_thresh = 0.7\n",
    "GPU = False\n",
    "\n",
    "# Initialize SuperPointFrontend\n",
    "fe = SuperPointFrontend(weights_path=weight_path,\n",
    "                        nms_dist=dist,\n",
    "                        conf_thresh=conf_thresh,\n",
    "                        nn_thresh=nn_thresh,\n",
    "                        cuda=GPU)\n",
    "\n",
    "# Run matching across 21 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for folder in img_folder:\n",
    "    name = folder.split('/')[-1]\n",
    "    outputFolder = os.path.join('Processed_Data/Feature Match',name)\n",
    "    matched = match_images_across_multiple(folder, outputFolder, fe, nn_thresh, save_matches=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matched[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SfM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycolmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpycolmap\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_sfm_with_colmap\u001b[39m(image_dir, output_dir, ply_point_cloud\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Initialize COLMAP's SfM\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycolmap'"
     ]
    }
   ],
   "source": [
    "import pycolmap\n",
    "import os\n",
    "\n",
    "def run_sfm_with_colmap(image_dir, output_dir, ply_point_cloud=None):\n",
    "    # Initialize COLMAP's SfM\n",
    "    recon = pycolmap.Reconstruction()\n",
    "\n",
    "    # Step 1: Feature extraction\n",
    "    pycolmap.extract_features(image_path=image_dir, database_path=f'{output_dir}/database.db')\n",
    "\n",
    "    # Step 2: Feature matching\n",
    "    pycolmap.match_sequential(database_path=f'{output_dir}/database.db')\n",
    "\n",
    "    # Step 3: Run SfM pipeline\n",
    "    recon.sparse_reconstruction(database_path=f'{output_dir}/database.db', \n",
    "                                image_dir=image_dir,\n",
    "                                output_dir=output_dir)\n",
    "    \n",
    "    # Step 4: Optionally load your training point cloud\n",
    "    if ply_point_cloud:\n",
    "        recon.import_points(ply_point_cloud)\n",
    "\n",
    "    # Step 5: Export the final point cloud\n",
    "    recon.export_ply(os.path.join(output_dir, 'dense_point_cloud.ply'))\n",
    "\n",
    "    return recon\n",
    "\n",
    "# Set directories\n",
    "image_dir = \"Processed_Data/360deg_img\"\n",
    "output_dir = \"SfM_Output\"\n",
    "ply_point_cloud = \"path_to_your_point_cloud.ply\"  # Optional, if used\n",
    "\n",
    "# Run SfM pipeline\n",
    "sfm_result = run_sfm_with_colmap('Processed_Data/360deg_img/0000', 'Processed_Data', 'Processed_Data/3D_PointCloud/0000.ply')\n",
    "print(\"SfM completed. Dense point cloud saved to\", os.path.join(output_dir, 'dense_point_cloud.ply'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from plyfile import PlyData\n",
    "import PIL\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import open3d as o3d\n",
    "import lightning as L\n",
    "from lightning.fabric import Fabric\n",
    "from torch.amp import autocast, GradScaler\n",
    "from functools import partial\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Set device and configure PyTorch settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True\n",
    "\n",
    "# Directory paths\n",
    "image_dir = \"/content/3D-Waifu-Model-Generator/Processed_Data/360deg_img\"\n",
    "point_cloud_dir = \"/content/3D-Waifu-Model-Generator/Processed_Data/3D_PointCloud\"\n",
    "\n",
    "class OptimizedMVSNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OptimizedMVSNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(4, 32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(8, 64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.gn3 = nn.GroupNorm(16, 128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.gn4 = nn.GroupNorm(32, 256)\n",
    "\n",
    "        # Depth prediction layer\n",
    "        self.depth_pred = nn.Conv2d(256, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_images, height, width, channels = x.size()\n",
    "        x = x.view(batch_size * num_images, channels, height, width)\n",
    "\n",
    "        # Apply convolutional layers with in-place ReLU, GroupNorm, and gradient checkpointing\n",
    "        x = F.relu(checkpoint(self._forward_block, self.conv1, self.gn1, x), inplace=True)\n",
    "        x = F.relu(checkpoint(self._forward_block, self.conv2, self.gn2, x), inplace=True)\n",
    "        x = F.relu(checkpoint(self._forward_block, self.conv3, self.gn3, x), inplace=True)\n",
    "        x = F.relu(checkpoint(self._forward_block, self.conv4, self.gn4, x), inplace=True)\n",
    "\n",
    "        depth = self.depth_pred(x)\n",
    "        depth = depth.view(batch_size, num_images, 1, height, width)\n",
    "        return depth\n",
    "\n",
    "    def _forward_block(self, conv, norm, x):\n",
    "        return norm(conv(x))\n",
    "\n",
    "def depth_loss(predicted_depth, ground_truth_depth):\n",
    "    return F.mse_loss(predicted_depth, ground_truth_depth, reduction='mean')\n",
    "\n",
    "def train_mvsnet(dataloader, point_cloud_dir, num_epochs=10, lr=5e-5):\n",
    "    fabric = Fabric(accelerator='cuda', devices=1, precision='bf16-true')\n",
    "    fabric.launch()\n",
    "    \n",
    "    with fabric.init_module():\n",
    "        model = OptimizedMVSNet().to(torch.bfloat16)  # Use bfloat16 for memory efficiency\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)  # AdamW is more memory efficient\n",
    "    num_steps = num_epochs * len(dataloader)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
    "    \n",
    "    model, optimizer = fabric.setup(model, optimizer)\n",
    "    dataloader = fabric.setup_dataloaders(dataloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            images, ground_truth_depth = batch\n",
    "            images = images.to(device)\n",
    "            ground_truth_depth = ground_truth_depth.to(device)\n",
    "\n",
    "            # Adjust the ground_truth_depth to match the model output shape\n",
    "            #ground_truth_depth = ground_truth_depth.unsqueeze(1).expand(-1, 21, -1, -1)  # Shape: [batch_size, 21, 1, height, width]\n",
    "\n",
    "            # Mixed-precision forward pass\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16):  # Ensure bfloat16 mixed precision\n",
    "                predicted_depth = model(images)\n",
    "                loss = depth_loss(predicted_depth, ground_truth_depth)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            fabric.backward(loss)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    torch.save(model.state_dict(), '/content/3D-Waifu-Model-Generator/optimized_mvsnet.pth')\n",
    "    print(\"Model saved as 'optimized_mvsnet.pth'\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_depth_map_from_point_cloud_orthographic(point_cloud, image_shape):\n",
    "    points = np.asarray(point_cloud.points)\n",
    "\n",
    "    # Normalize the points to fit the image space\n",
    "    x_min, x_max = points[:, 0].min(), points[:, 0].max()\n",
    "    y_min, y_max = points[:, 1].min(), points[:, 1].max()\n",
    "    z_min, z_max = points[:, 2].min(), points[:, 2].max()\n",
    "\n",
    "    # Create an empty depth map\n",
    "    depth_map = np.full(image_shape, np.inf)\n",
    "\n",
    "    # Convert point cloud coordinates to image coordinates\n",
    "    height, width = image_shape\n",
    "    x_image = np.clip(((points[:, 0] - x_min) / (x_max - x_min) * width).astype(int), 0, width - 1)\n",
    "    y_image = np.clip(((points[:, 1] - y_min) / (y_max - y_min) * height).astype(int), 0, height - 1)\n",
    "\n",
    "    # Assign Z values to pixels\n",
    "    for i in range(points.shape[0]):\n",
    "        depth_value = points[i, 2]\n",
    "        if depth_map[y_image[i], x_image[i]] > depth_value:\n",
    "            depth_map[y_image[i], x_image[i]] = depth_value\n",
    "\n",
    "    depth_map[depth_map == np.inf] = 0\n",
    "    return torch.tensor(depth_map, dtype=torch.float32)\n",
    "\n",
    "def load_point_cloud_to_depth_map(point_cloud_dir, image_shape):\n",
    "    point_cloud_files = [os.path.join(point_cloud_dir, f).replace(\"\\\\\", \"/\") for f in os.listdir(point_cloud_dir) if f.endswith('.ply')]\n",
    "    depth_maps = []\n",
    "\n",
    "    for pc_file in point_cloud_files:\n",
    "        pcd = o3d.io.read_point_cloud(pc_file)\n",
    "        \n",
    "        # Generate the depth map\n",
    "        depth_map = create_depth_map_from_point_cloud_orthographic(pcd, image_shape)\n",
    "\n",
    "        # Resize depth map to match the model output\n",
    "        depth_map_tensor = depth_map.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W] shape\n",
    "        resized_depth_map = F.interpolate(depth_map_tensor, size=(576, 576), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        depth_maps.append(resized_depth_map.squeeze(0))  # Remove the batch dimension\n",
    "\n",
    "    return torch.stack(depth_maps)  # Stack all depth maps into a tensor\n",
    "\n",
    "def create_dataloader(image_dir, point_cloud_dir, batch_size=1):\n",
    "    images = load_images(image_dir)\n",
    "    ground_truth_depth_maps = load_point_cloud_to_depth_map(point_cloud_dir, images[0].shape)\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(images, dtype=torch.float32), ground_truth_depth_maps)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "def load_images(image_paths):\n",
    "    output = []\n",
    "    path = glob.glob(f\"{image_paths}/*\")\n",
    "    for dir in path:\n",
    "        images = []\n",
    "        for path in glob.glob(f\"{dir}/*\"):\n",
    "            img = PIL.Image.open(path)\n",
    "            images.append(np.array(img))\n",
    "        output.append(images)\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "\n",
    "\n",
    "def load_images(image_paths):\n",
    "    output = []\n",
    "    path = glob.glob(f\"{image_paths}/*\")\n",
    "    for dir in path:\n",
    "        images = []\n",
    "        for path in glob.glob(f\"{dir}/*\"):\n",
    "            path = str(path).replace(\"\\\\\",'/')\n",
    "            #print(path)\n",
    "            img = PIL.Image.open(path)\n",
    "            images.append(img)\n",
    "        output.append(images)\n",
    "        print(np.array(output).shape)\n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 21, 576, 576, 3)\n",
      "(2, 21, 576, 576, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_dir = \"Processed_Data/360deg_img\"\n",
    "point_cloud_dir = \"Processed_Data/3D_PointCloud\"\n",
    "\n",
    "images = load_images(image_dir)\n",
    "ground_truth_depth_maps = load_point_cloud_to_depth_map(point_cloud_dir, (576, 576))\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(torch.tensor(images, dtype=torch.float32), ground_truth_depth_maps)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "# Set directories\n",
    "image_dir = \"Processed_Data/360deg_img\"\n",
    "point_cloud_dir = \"Training_Point_Clouds\"\n",
    "output_dir = \"\"\n",
    "\n",
    "# Run MVS pipeline\n",
    "model = train_mvsnet(dataloader, point_cloud_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from plyfile import PlyData\n",
    "import PIL\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import open3d as o3d\n",
    "import lightning as L\n",
    "from lightning.fabric import Fabric\n",
    "from torch.amp import autocast, GradScaler\n",
    "from functools import partial\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "# Set device and configure PyTorch settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = True\n",
    "\n",
    "# Directory paths\n",
    "image_dir = \"/content/3D-Waifu-Model-Generator/Processed_Data/360deg_img\"\n",
    "point_cloud_dir = \"/content/3D-Waifu-Model-Generator/Processed_Data/3D_PointCloud\"\n",
    "\n",
    "class PointCloudMVSNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PointCloudMVSNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(4, 32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(8, 64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.gn3 = nn.GroupNorm(16, 128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.gn4 = nn.GroupNorm(32, 256)\n",
    "\n",
    "        # Prediction layers\n",
    "        self.coord_pred = nn.Conv2d(256, 3, kernel_size=3, padding=1)  # Predict x, y, z coordinates\n",
    "        self.rgb_pred = nn.Conv2d(256, 3, kernel_size=3, padding=1)    # Predict RGB values\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_images, height, width, channels = x.size()\n",
    "        x = x.view(batch_size * num_images, channels, height, width)\n",
    "\n",
    "        # Apply convolutional layers\n",
    "        x = F.relu(checkpoint(self._forward_block, self.conv1, self.gn1, x), inplace=True)\n",
    "        x = F.relu(checkpoint(self._forward_block, self.conv2, self.gn2, x), inplace=True)\n",
    "        x = F.relu(checkpoint(self._forward_block, self.conv3, self.gn3, x), inplace=True)\n",
    "        x = F.relu(checkpoint(self._forward_block, self.conv4, self.gn4, x), inplace=True)\n",
    "\n",
    "        # Predict 3D coordinates and RGB values\n",
    "        coords = self.coord_pred(x)  # [batch_size * num_images, 3, height, width]\n",
    "        rgb = self.rgb_pred(x)       # [batch_size * num_images, 3, height, width]\n",
    "\n",
    "        # Reshape to get final shape as [batch_size, num_images, height * width, 6]\n",
    "        coords = coords.view(batch_size, num_images, 3, height * width)  # Flatten spatial dimensions\n",
    "        rgb = rgb.view(batch_size, num_images, 3, height * width)        # Flatten spatial dimensions\n",
    "        \n",
    "        # Combine coordinates and RGB\n",
    "        point_cloud = torch.cat([coords, rgb], dim=2)  # [batch_size, num_images, 6, height * width]\n",
    "        \n",
    "        return point_cloud.permute(0, 2, 1, 3).contiguous().view(batch_size, 6, -1)  # [batch_size, 6, num_points]\n",
    "    \n",
    "    def _forward_block(self, conv, norm, x):\n",
    "        return norm(conv(x))\n",
    "\n",
    "def point_cloud_loss(predicted_coords, predicted_rgb, ground_truth_coords, ground_truth_rgb):\n",
    "    coord_loss = F.mse_loss(predicted_coords, ground_truth_coords, reduction='mean')  # Loss for coordinates\n",
    "    rgb_loss = F.mse_loss(predicted_rgb, ground_truth_rgb, reduction='mean')  # Loss for RGB values\n",
    "    return coord_loss + rgb_loss\n",
    "\n",
    "def train_mvsnet(dataloader, point_cloud_dir, num_epochs=10, lr=0.001):\n",
    "    fabric = Fabric(accelerator='cuda', devices=1, precision='bf16-true')\n",
    "    fabric.launch()\n",
    "    \n",
    "    with fabric.init_module():\n",
    "        model = PointCloudMVSNet().to(torch.bfloat16)  # Use bfloat16 for memory efficiency\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)  \n",
    "    num_steps = num_epochs * len(dataloader)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps)\n",
    "    \n",
    "    model, optimizer = fabric.setup(model, optimizer)\n",
    "    dataloader = fabric.setup_dataloaders(dataloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in dataloader:\n",
    "            images, ground_truth = batch\n",
    "            ground_truth_coords, ground_truth_rgb = ground_truth[:, :, :3], ground_truth[:, :, 3:]\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Mixed-precision forward pass\n",
    "            with autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                predicted = model(images)\n",
    "                predicted_coords = predicted[:, :, :3]\n",
    "                predicted_rgb = predicted[:, :, 3:]\n",
    "                ground_truth_coords = ground_truth_coords.to(device)\n",
    "                ground_truth_rgb = ground_truth_rgb.to(device)\n",
    "\n",
    "                loss = point_cloud_loss(predicted_coords, predicted_rgb, ground_truth_coords, ground_truth_rgb)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            fabric.backward(loss)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss:.4f}')\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    torch.save(model.state_dict(), '/content/3D-Waifu-Model-Generator/point_cloud_mvsnet.pth')\n",
    "    print(\"Model saved as 'point_cloud_mvsnet.pth'\")\n",
    "    return model\n",
    "\n",
    "def load_point_cloud_with_rgb(point_cloud_dir, image_shape):\n",
    "    point_cloud_files = [os.path.join(point_cloud_dir, f).replace(\"\\\\\", \"/\") for f in os.listdir(point_cloud_dir) if f.endswith('.ply')]\n",
    "    point_clouds = []\n",
    "\n",
    "    for pc_file in point_cloud_files:\n",
    "        pcd = o3d.io.read_point_cloud(pc_file)\n",
    "        points = np.asarray(pcd.points)\n",
    "        colors = np.asarray(pcd.colors)  # Extract RGB values\n",
    "\n",
    "        point_cloud = np.concatenate([points, colors], axis=1)  # Concatenate [x, y, z] with [r, g, b]\n",
    "        point_clouds.append(point_cloud)\n",
    "\n",
    "    return torch.tensor(point_clouds, dtype=torch.float32)\n",
    "def generate_point_cloud_from_model(model, images):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_point_cloud = model(images.to(device))\n",
    "        # predicted_point_cloud will have shape [batch_size, num_points, 6]\n",
    "        return predicted_point_cloud\n",
    "\n",
    "\n",
    "def create_dataloader(image_dir, point_cloud_dir, batch_size=1):\n",
    "    images = load_images(image_dir)\n",
    "    pcds = load_point_cloud_with_rgb(point_cloud_dir, images[0].shape)\n",
    "    dataset = torch.utils.data.TensorDataset(torch.tensor(images, dtype=torch.float32), ground_truth_depth_maps)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "def load_images(image_paths):\n",
    "    output = []\n",
    "    path = glob.glob(f\"{image_paths}/*\")\n",
    "    for dir in path:\n",
    "        images = []\n",
    "        for path in glob.glob(f\"{dir}/*\"):\n",
    "            img = PIL.Image.open(path)\n",
    "            images.append(np.array(img))\n",
    "        output.append(images)\n",
    "    return np.array(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_images(\"Processed_Data/360deg_img\")\n",
    "#pcds = load_point_cloud_with_rgb(\"Processed_Data\\3D_PointCloud\", images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 2,  0,  3],\n",
       "          [ 0,  1,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  2],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 2,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 1,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  1,  1],\n",
       "          [ 0,  0,  3],\n",
       "          [ 0,  0,  1],\n",
       "          ...,\n",
       "          [ 2,  0,  2],\n",
       "          [ 0,  0,  2],\n",
       "          [ 5,  0,  3]]],\n",
       "\n",
       "\n",
       "        [[[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 1,  0,  3],\n",
       "          [ 0,  1,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  2],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 1,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 1,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  1,  1],\n",
       "          [ 0,  0,  2],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 2,  0,  2],\n",
       "          [ 0,  0,  2],\n",
       "          [ 4,  0,  3]]],\n",
       "\n",
       "\n",
       "        [[[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 1,  0,  2],\n",
       "          [ 0,  1,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  2],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  1,  0],\n",
       "          [ 0,  0,  2],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 1,  0,  2],\n",
       "          [ 0,  0,  2],\n",
       "          [ 3,  0,  3]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  2],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  2],\n",
       "          [ 3,  0,  3]]],\n",
       "\n",
       "\n",
       "        [[[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 1,  0,  3],\n",
       "          [ 0,  1,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  2],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 1,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 1,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  1,  1],\n",
       "          [ 0,  0,  2],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 2,  0,  2],\n",
       "          [ 0,  0,  2],\n",
       "          [ 4,  0,  3]]],\n",
       "\n",
       "\n",
       "        [[[ 1,  3,  1],\n",
       "          [ 0,  0,  0],\n",
       "          [ 1,  0,  1],\n",
       "          ...,\n",
       "          [ 0,  1,  1],\n",
       "          [ 5,  0,  6],\n",
       "          [ 3,  4,  0]],\n",
       "\n",
       "         [[ 2,  1,  1],\n",
       "          [ 3,  0,  0],\n",
       "          [ 2,  0,  3],\n",
       "          ...,\n",
       "          [ 0,  1,  1],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 3,  3,  2],\n",
       "          [ 4,  0,  3],\n",
       "          [ 0,  0,  4],\n",
       "          ...,\n",
       "          [ 2,  0,  2],\n",
       "          [ 3,  0,  4],\n",
       "          [ 2,  1,  3]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1,  1,  1],\n",
       "          [ 2,  0,  4],\n",
       "          [ 2,  2,  2],\n",
       "          ...,\n",
       "          [ 3,  2,  3],\n",
       "          [ 3,  3,  3],\n",
       "          [ 5,  2,  2]],\n",
       "\n",
       "         [[ 3,  1,  3],\n",
       "          [ 5,  0,  3],\n",
       "          [ 4,  2,  2],\n",
       "          ...,\n",
       "          [ 6,  2,  1],\n",
       "          [ 3,  2,  4],\n",
       "          [ 4,  1,  1]],\n",
       "\n",
       "         [[ 0,  4,  3],\n",
       "          [ 0,  0,  3],\n",
       "          [ 2,  0,  2],\n",
       "          ...,\n",
       "          [ 6,  1,  4],\n",
       "          [ 3,  1,  5],\n",
       "          [ 9,  2,  5]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 1,  0,  2],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 1,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 1,  0,  1],\n",
       "          [ 0,  0,  2],\n",
       "          [ 3,  0,  2]]],\n",
       "\n",
       "\n",
       "        [[[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 1,  0,  2],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  1,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 1,  0,  1],\n",
       "          [ 0,  0,  2],\n",
       "          [ 3,  0,  1]]],\n",
       "\n",
       "\n",
       "        [[[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 1,  0,  2],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  1,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  2],\n",
       "          [ 3,  0,  2]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  1,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  1],\n",
       "          [ 3,  0,  2]]],\n",
       "\n",
       "\n",
       "        [[[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 2,  0,  3],\n",
       "          [ 0,  1,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 0,  0,  2],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 0,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  0,  0],\n",
       "          [ 1,  0,  0],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 1,  0,  0],\n",
       "          [ 0,  0,  1],\n",
       "          [ 0,  0,  0]],\n",
       "\n",
       "         [[ 0,  2,  1],\n",
       "          [ 0,  0,  2],\n",
       "          [ 0,  0,  0],\n",
       "          ...,\n",
       "          [ 2,  0,  2],\n",
       "          [ 0,  0,  3],\n",
       "          [ 4,  0,  3]]],\n",
       "\n",
       "\n",
       "        [[[ 0,  1,  0],\n",
       "          [ 0,  0,  0],\n",
       "          [ 1,  0,  0],\n",
       "          ...,\n",
       "          [ 2,  2,  3],\n",
       "          [ 6,  1,  6],\n",
       "          [ 4,  4,  0]],\n",
       "\n",
       "         [[ 1,  1,  0],\n",
       "          [ 2,  0,  0],\n",
       "          [ 2,  0,  3],\n",
       "          ...,\n",
       "          [ 0,  1,  1],\n",
       "          [ 0,  0,  1],\n",
       "          [ 1,  0,  0]],\n",
       "\n",
       "         [[ 2,  2,  1],\n",
       "          [ 3,  0,  1],\n",
       "          [ 0,  0,  3],\n",
       "          ...,\n",
       "          [ 2,  0,  2],\n",
       "          [ 3,  0,  4],\n",
       "          [ 3,  1,  2]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2,  1,  0],\n",
       "          [ 3,  0,  4],\n",
       "          [ 3,  2,  3],\n",
       "          ...,\n",
       "          [ 3,  3,  3],\n",
       "          [ 4,  3,  4],\n",
       "          [ 6,  2,  3]],\n",
       "\n",
       "         [[ 4,  1,  2],\n",
       "          [ 5,  0,  2],\n",
       "          [ 4,  1,  2],\n",
       "          ...,\n",
       "          [ 7,  3,  2],\n",
       "          [ 3,  2,  4],\n",
       "          [ 6,  3,  2]],\n",
       "\n",
       "         [[ 0,  4,  3],\n",
       "          [ 0,  0,  4],\n",
       "          [ 2,  0,  2],\n",
       "          ...,\n",
       "          [ 7,  2,  5],\n",
       "          [ 4,  3,  6],\n",
       "          [11,  3,  5]]]]], dtype=uint8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpoint_cloud_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m train_mvsnet(dataloader,point_cloud_dir)\n",
      "Cell \u001b[1;32mIn[24], line 140\u001b[0m, in \u001b[0;36mcreate_dataloader\u001b[1;34m(image_dir, point_cloud_dir, batch_size)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataloader\u001b[39m(image_dir, point_cloud_dir, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    139\u001b[0m     images \u001b[38;5;241m=\u001b[39m load_images(image_dir)\n\u001b[1;32m--> 140\u001b[0m     pcds \u001b[38;5;241m=\u001b[39m load_point_cloud_with_rgb(point_cloud_dir, \u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    141\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mTensorDataset(torch\u001b[38;5;241m.\u001b[39mtensor(images, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), ground_truth_depth_maps)\n\u001b[0;32m    142\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(\"Processed_Data\\360deg_img\",\"\")\n",
    "model = train_mvsnet(dataloader,point_cloud_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patar\\AppData\\Local\\Temp\\ipykernel_23280\\1417420814.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('optimized_mvsnet.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OptimizedMVSNet()\n",
    "model.load_state_dict(torch.load('optimized_mvsnet.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"Processed_Data/360deg_img\"\n",
    "point_cloud_dir = \"Processed_Data/3D_PointCloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 21, 576, 576, 3])\n",
      "torch.Size([1, 21, 576, 576, 3])\n"
     ]
    }
   ],
   "source": [
    "for a , b in dataloader:\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "C:\\Users\\patar\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_depth = model(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=predicted_depth.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (2960779117.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    for depth_map in predicted_depth.squeeze(0)\u001b[0m\n\u001b[1;37m                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "for depth_map in predicted_depth.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " len([)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "# Function to create a point cloud from a depth map\n",
    "def create_point_cloud(depth_map):\n",
    "    height, width = depth_map.shape\n",
    "    x, y = np.meshgrid(np.arange(width), np.arange(height))\n",
    "\n",
    "    # Convert pixel coordinates to camera coordinates\n",
    "    z = depth_map.flatten()  # Depth values\n",
    "    x = x.flatten()  # X pixel coordinates\n",
    "    y = y.flatten()  # Y pixel coordinates\n",
    "\n",
    "    # Stack the coordinates (assuming a scale factor of 1 for simplicity)\n",
    "    points = np.vstack((x, y, z)).T\n",
    "    valid_points = points[depth_map.flatten() > 0]  # Filter out points with zero depth\n",
    "    return o3d.geometry.PointCloud(o3d.utility.Vector3dVector(valid_points))\n",
    "\n",
    "# Load your depth maps\n",
    "depth_map_files = [depth_map.squeeze(0).numpy() for depth_map in predicted_depth.squeeze(0)] # Replace with your depth map file paths\n",
    "\n",
    "# Create point clouds\n",
    "point_clouds = []\n",
    "for depth_map in depth_map_files:\n",
    "    point_cloud = create_point_cloud(depth_map)\n",
    "    \n",
    "    # Optionally apply a transformation if needed (for example, if you have known camera poses)\n",
    "    # point_cloud.transform(transformation_matrix)  # Define your transformation matrix if available\n",
    "    \n",
    "    point_clouds.append(point_cloud)\n",
    "    break\n",
    "\n",
    "# Merge point clouds into a single point cloud\n",
    "merged_point_cloud = o3d.geometry.PointCloud()\n",
    "for pc in point_clouds:\n",
    "    merged_point_cloud += pc\n",
    "\n",
    "# Optional: Estimate normals for better visualization\n",
    "merged_point_cloud.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "\n",
    "# Optional: Create a mesh using Poisson reconstruction\n",
    "mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(merged_point_cloud, depth=9)\n",
    "\n",
    "# Visualize the merged point cloud or mesh\n",
    "o3d.visualization.draw_geometries([merged_point_cloud])\n",
    "# o3d.visualization.draw_geometries([mesh])\n",
    "\n",
    "# Save the merged point cloud or mesh\n",
    "o3d.io.write_point_cloud(\"merged_point_cloud.ply\", merged_point_cloud)\n",
    "o3d.io.write_triangle_mesh(\"reconstructed_mesh.ply\", mesh)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
